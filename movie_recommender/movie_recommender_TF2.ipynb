{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 来说说数据预处理\n",
    "\n",
    "- UserID、Occupation和MovieID不用变。\n",
    "- Gender字段：需要将‘F’和‘M’转换成0和1。\n",
    "- Age字段：要转成7个连续数字0~6。\n",
    "- Genres字段：是分类字段，要转成数字。首先将Genres中的类别转成字符串到数字的字典，然后再将每个电影的Genres字段转成数字列表，因为有些电影是多个Genres的组合。\n",
    "- Title字段：处理方式跟Genres字段一样，首先创建文本到数字的字典，然后将Title中的描述转成数字的列表。另外Title中的年份也需要去掉。\n",
    "- Genres和Title字段需要将长度统一，这样在神经网络中方便处理。空白部分用‘< PAD >’对应的数字填充。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\envs\\dl\\lib\\site-packages\\numpy\\core\\__init__.py:29: UserWarning: loaded more than 1 DLL from .libs:\n",
      "E:\\Anaconda3\\envs\\dl\\lib\\site-packages\\numpy\\.libs\\libopenblas.IPBC74C7KURV7CB2PKT5Z5FNR3SIBV4J.gfortran-win_amd64.dll\n",
      "E:\\Anaconda3\\envs\\dl\\lib\\site-packages\\numpy\\.libs\\libopenblas.TXA6YQSD3GCQQC22GEQ54J2UDCXDXHWN.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import math_ops\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    加载数据集\n",
    "    \"\"\"\n",
    "    #读取User数据\n",
    "    users_title = ['UserID', 'Gender', 'Age', 'JobID', 'Zip-code']\n",
    "    users = pd.read_csv('./ml-1m/ml-1m/users.dat',\n",
    "                        sep='::',\n",
    "                        header=None,\n",
    "                        names=users_title,\n",
    "                        engine='python')\n",
    "    users = users.filter(regex='UserID|Gender|Age|JobID')\n",
    "    users_orig = users.values\n",
    "    #改变User数据中性别和年龄\n",
    "    gender_map = {'F': 0, 'M': 1}\n",
    "    users['Gender'] = users['Gender'].map(gender_map)\n",
    "\n",
    "    age_map = {val: ii for ii, val in enumerate(set(users['Age']))}\n",
    "    users['Age'] = users['Age'].map(age_map)\n",
    "\n",
    "    #读取Movie数据集\n",
    "    movies_title = ['MovieID', 'Title', 'Genres']\n",
    "    movies = pd.read_csv('./ml-1m/ml-1m/movies.dat',\n",
    "                         sep='::',\n",
    "                         header=None,\n",
    "                         names=movies_title,\n",
    "                         engine='python')\n",
    "    movies_orig = movies.values\n",
    "    #将Title中的年份去掉\n",
    "    pattern = re.compile(r'^(.*)\\((\\d+)\\)$')\n",
    "\n",
    "    title_map = {\n",
    "        val: pattern.match(val).group(1)\n",
    "        for ii, val in enumerate(set(movies['Title']))\n",
    "    }\n",
    "    movies['Title'] = movies['Title'].map(title_map)\n",
    "\n",
    "    #电影类型转数字字典\n",
    "    genres_set = set()\n",
    "    for val in movies['Genres'].str.split('|'):\n",
    "        genres_set.update(val)\n",
    "\n",
    "    genres_set.add('<PAD>')\n",
    "    genres2int = {val: ii for ii, val in enumerate(genres_set)}\n",
    "\n",
    "    #将电影类型转成等长数字列表，长度是18\n",
    "    genres_map = {\n",
    "        val: [genres2int[row] for row in val.split('|')]\n",
    "        for ii, val in enumerate(set(movies['Genres']))\n",
    "    }\n",
    "\n",
    "    for key in genres_map:\n",
    "        for cnt in range(max(genres2int.values()) - len(genres_map[key])):\n",
    "            genres_map[key].insert(\n",
    "                len(genres_map[key]) + cnt, genres2int['<PAD>'])\n",
    "\n",
    "    movies['Genres'] = movies['Genres'].map(genres_map)\n",
    "\n",
    "    #电影Title转数字字典\n",
    "    title_set = set()\n",
    "    for val in movies['Title'].str.split():\n",
    "        title_set.update(val)\n",
    "\n",
    "    title_set.add('<PAD>')\n",
    "    title2int = {val: ii for ii, val in enumerate(title_set)}\n",
    "\n",
    "    #将电影Title转成等长数字列表，长度是15\n",
    "    title_count = 15\n",
    "    title_map = {\n",
    "        val: [title2int[row] for row in val.split()]\n",
    "        for ii, val in enumerate(set(movies['Title']))\n",
    "    }\n",
    "\n",
    "    for key in title_map:\n",
    "        for cnt in range(title_count - len(title_map[key])):\n",
    "            title_map[key].insert(\n",
    "                len(title_map[key]) + cnt, title2int['<PAD>'])\n",
    "\n",
    "    movies['Title'] = movies['Title'].map(title_map)\n",
    "\n",
    "    #读取评分数据集\n",
    "    ratings_title = ['UserID', 'MovieID', 'ratings', 'timestamps']\n",
    "    ratings = pd.read_csv('./ml-1m/ml-1m/ratings.dat',\n",
    "                          sep='::',\n",
    "                          header=None,\n",
    "                          names=ratings_title,\n",
    "                          engine='python')\n",
    "    ratings = ratings.filter(regex='UserID|MovieID|ratings')\n",
    "\n",
    "    #合并三个表\n",
    "    data = pd.merge(pd.merge(ratings, users), movies)\n",
    "\n",
    "    #将数据分成X和y两张表\n",
    "    target_fields = ['ratings']\n",
    "    features_pd, targets_pd = data.drop(target_fields,\n",
    "                                        axis=1), data[target_fields]\n",
    "\n",
    "    features = features_pd.values\n",
    "    targets_values = targets_pd.values\n",
    "\n",
    "    return title_count, title_set, genres2int, features, targets_values, ratings, users, movies, data, movies_orig, users_orig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载数据并保存到本地\n",
    "\n",
    "- title_count：Title字段的长度（15）\n",
    "- title_set：Title文本的集合\n",
    "- genres2int：电影类型转数字的字典\n",
    "- features：是输入X\n",
    "- targets_values：是学习目标y\n",
    "- ratings：评分数据集的Pandas对象\n",
    "- users：用户数据集的Pandas对象\n",
    "- movies：电影数据的Pandas对象\n",
    "- data：三个数据集组合在一起的Pandas对象\n",
    "- movies_orig：没有做数据处理的原始电影数据\n",
    "- users_orig：没有做数据处理的原始用户数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_count, title_set, genres2int, features, targets_values, ratings, users, movies, data, movies_orig, users_orig = load_data()\n",
    "\n",
    "pickle.dump((title_count, title_set, genres2int, features, targets_values,\n",
    "             ratings, users, movies, data, movies_orig, users_orig),\n",
    "            open('preprocess.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预处理后数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>JobID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID  Gender  Age  JobID\n",
       "0       1       0    0     10\n",
       "1       2       1    5     16\n",
       "2       3       1    6     15\n",
       "3       4       1    2      7\n",
       "4       5       1    6     20"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MovieID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[2382, 3040, 4027, 4027, 4027, 4027, 4027, 402...</td>\n",
       "      <td>[4, 3, 2, 17, 17, 17, 17, 17, 17, 17, 17, 17, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[4563, 4027, 4027, 4027, 4027, 4027, 4027, 402...</td>\n",
       "      <td>[11, 3, 7, 17, 17, 17, 17, 17, 17, 17, 17, 17,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>[2783, 2857, 2260, 4027, 4027, 4027, 4027, 402...</td>\n",
       "      <td>[2, 16, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>[4340, 2199, 4214, 4027, 4027, 4027, 4027, 402...</td>\n",
       "      <td>[2, 8, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>[2447, 992, 2892, 1195, 1191, 4404, 4027, 4027...</td>\n",
       "      <td>[2, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MovieID                                              Title  \\\n",
       "0        1  [2382, 3040, 4027, 4027, 4027, 4027, 4027, 402...   \n",
       "1        2  [4563, 4027, 4027, 4027, 4027, 4027, 4027, 402...   \n",
       "2        3  [2783, 2857, 2260, 4027, 4027, 4027, 4027, 402...   \n",
       "3        4  [4340, 2199, 4214, 4027, 4027, 4027, 4027, 402...   \n",
       "4        5  [2447, 992, 2892, 1195, 1191, 4404, 4027, 4027...   \n",
       "\n",
       "                                              Genres  \n",
       "0  [4, 3, 2, 17, 17, 17, 17, 17, 17, 17, 17, 17, ...  \n",
       "1  [11, 3, 7, 17, 17, 17, 17, 17, 17, 17, 17, 17,...  \n",
       "2  [2, 16, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17...  \n",
       "3  [2, 8, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,...  \n",
       "4  [2, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1,\n",
       "       list([2382, 3040, 4027, 4027, 4027, 4027, 4027, 4027, 4027, 4027, 4027, 4027, 4027, 4027, 4027]),\n",
       "       list([4, 3, 2, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从本地读取数据\n",
    "title_count, title_set, genres2int, features, targets_values, ratings, users, movies, data, movies_orig, users_orig = pickle.load(\n",
    "    open('preprocess.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型设计  \n",
    "通常的处理是将这些字段转成one hot编码，但是像UserID、MovieID这样的字段就会变成非常的稀疏，输入的维度急剧膨胀  \n",
    "所以在预处理数据时将这些字段转成了数字，我们用这个数字当做嵌入矩阵的索引，在网络的第一层使用了嵌入层，维度是（N，32）和（N，16）  \n",
    "电影类型的处理要多一步，有时一个电影有多个电影类型，这样从嵌入矩阵索引出来是一个（n，32）的矩阵，因为有多个类型嘛，我们要将这个矩阵求和，变成（1，32）的向量  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文本卷积网络\n",
    "网络的第一层是词嵌入层，由每一个单词的嵌入向量组成的嵌入矩阵。下一层使用多个不同尺寸（窗口大小）的卷积核在嵌入矩阵上做卷积，窗口大小指的是每次卷积覆盖几个单词。这里跟对图像做卷积不太一样，图像的卷积通常用2x2、3x3、5x5之类的尺寸，而文本卷积要覆盖整个单词的嵌入向量，所以尺寸是（单词数，向量维度），比如每次滑动3个，4个或者5个单词。第三层网络是max pooling得到一个长向量，最后使用dropout做正则化，最终得到了电影Title的特征。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 辅助函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_params(params):\n",
    "    '''\n",
    "    参数保存到文件\n",
    "    '''\n",
    "    pickle.dump(params, open('params.p', 'wb'))\n",
    "\n",
    "\n",
    "def load_params():\n",
    "    '''\n",
    "    参数保存到文件\n",
    "    '''\n",
    "    return pickle.load(open('params.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 编码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 嵌入矩阵维度\n",
    "embed_dim = 32\n",
    "# 用户ID个数\n",
    "uid_max = max(features.take(0, 1)) + 1 # 6040\n",
    "# 性别个数 \n",
    "gender_max = max(features.take(2, 1)) + 1 # 1 + 1 = 2\n",
    "# 年龄类别个数\n",
    "age_max = max(features.take(3, 1)) + 1 # 6 + 1 = 7\n",
    "# 职业个数\n",
    "job_max = max(features.take(4, 1)) + 1 # 20 + 1 = 21\n",
    "\n",
    "# 电影ID个数\n",
    "movie_id_max = max(features.take(1, 1)) + 1 # 3952\n",
    "# 电影类型地个数\n",
    "movie_categories_max = max(genres2int.values()) + 1 # 18 +1 + 19\n",
    "# 电影名单词个数\n",
    "movie_title_max = len(title_set) # 5216\n",
    "\n",
    "# 对电影类型嵌入向量做加和操作地标志\n",
    "combiner = 'sum'\n",
    "\n",
    "# 电影名长度\n",
    "sentences_size = title_count # 15\n",
    "# 文本卷积滑动窗口， 分别滑动2， 3， 4， 5个单词\n",
    "window_sizes = {2, 3, 4, 5}\n",
    "# 文本卷积核数量\n",
    "filter_num = 8\n",
    "# 电影ID转下标地字典，数据集中电影ID跟下标不一致\n",
    "movieid2idx = {val[0]:i for i, val in enumerate(movies.values)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 超参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "num_epochs = 5\n",
    "batch_size = 256\n",
    "\n",
    "dropout_keep = 0.5\n",
    "learning_rate = 0.001\n",
    "\n",
    "show_every_n_batch = 20\n",
    "\n",
    "save_dir = './save'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 输入\n",
    "定义输入占位符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inputs():\n",
    "    uid = tf.keras.layers.Input(shape=(1, ), dtype='int32', name='uid')\n",
    "    user_gender = tf.keras.layers.Input(shape=(1, ),\n",
    "                                        dtype='int32',\n",
    "                                        name='user_gender')\n",
    "    user_age = tf.keras.layers.Input(shape=(1, ),\n",
    "                                     dtype='int32',\n",
    "                                     name='user_age')\n",
    "    user_job = tf.keras.layers.Input(shape=(1, ),\n",
    "                                     dtype='int32',\n",
    "                                     name='user_job')\n",
    "\n",
    "    movie_id = tf.keras.layers.Input(shape=(1, ),\n",
    "                                     dtype='int32',\n",
    "                                     name='movie_id')\n",
    "    movie_categories = tf.keras.layers.Input(shape=(18, ),\n",
    "                                             dtype='int32',\n",
    "                                             name='movie_categories')\n",
    "    movie_titles = tf.keras.layers.Input(shape=(15, ),\n",
    "                                         dtype='int32',\n",
    "                                         name='movie_titles')\n",
    "    return uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建神经网络\n",
    "定义User地嵌入矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_embedding(uid, user_gender, user_age, user_job):\n",
    "    uid_embed_layer = tf.keras.layers.Embedding(uid_max,\n",
    "                                                embed_dim,\n",
    "                                                input_length=1,\n",
    "                                                name='uid_embed_layer')(uid)\n",
    "    gender_embed_layer = tf.keras.layers.Embedding(\n",
    "        gender_max, embed_dim // 2, input_length=1,\n",
    "        name='gender_embed_layer')(user_gender)\n",
    "    age_embed_layer = tf.keras.layers.Embedding(\n",
    "        age_max, embed_dim // 2, input_length=1,\n",
    "        name='age_embed_layer')(user_age)\n",
    "    job_embed_layer = tf.keras.layers.Embedding(\n",
    "        job_max, embed_dim // 2, input_length=1,\n",
    "        name='job_embed_layer')(user_job)\n",
    "    return uid_embed_layer, gender_embed_layer, age_embed_layer, job_embed_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User地嵌入矩阵一起全连接生成User地特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_feature_layer(uid_embed_layer, gender_embed_layer,\n",
    "                           age_embed_layer, job_embed_layer):\n",
    "    #第一层全连接\n",
    "    uid_fc_layer = tf.keras.layers.Dense(embed_dim,\n",
    "                                         name=\"uid_fc_layer\",\n",
    "                                         activation='relu')(uid_embed_layer)\n",
    "    gender_fc_layer = tf.keras.layers.Dense(\n",
    "        embed_dim, name=\"gender_fc_layer\",\n",
    "        activation='relu')(gender_embed_layer)\n",
    "    age_fc_layer = tf.keras.layers.Dense(embed_dim,\n",
    "                                         name=\"age_fc_layer\",\n",
    "                                         activation='relu')(age_embed_layer)\n",
    "    job_fc_layer = tf.keras.layers.Dense(embed_dim,\n",
    "                                         name=\"job_fc_layer\",\n",
    "                                         activation='relu')(job_embed_layer)\n",
    "\n",
    "    #第二层全连接\n",
    "    user_combine_layer = tf.keras.layers.concatenate(\n",
    "        [uid_fc_layer, gender_fc_layer, age_fc_layer, job_fc_layer],\n",
    "        2)  #(?, 1, 128)\n",
    "    user_combine_layer = tf.keras.layers.Dense(200, activation='tanh')(\n",
    "        user_combine_layer)  #(?, 1, 200)\n",
    "\n",
    "    user_combine_layer_flat = tf.keras.layers.Reshape(\n",
    "        [200], name=\"user_combine_layer_flat\")(user_combine_layer)\n",
    "    return user_combine_layer, user_combine_layer_flat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义Movie ID地嵌入矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_id_embed_layer(movie_id):\n",
    "    movie_id_embed_layer = tf.keras.layers.Embedding(\n",
    "        movie_id_max, embed_dim, input_length=1,\n",
    "        name='movie_id_embed_layer')(movie_id)\n",
    "    return movie_id_embed_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "合并电影类型地多个嵌入向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_categories_layers(movie_categories):\n",
    "    movie_categories_embed_layer = tf.keras.layers.Embedding(\n",
    "        movie_categories_max,\n",
    "        embed_dim,\n",
    "        input_length=18,\n",
    "        name='movie_categories_embed_layer')(movie_categories)\n",
    "    movie_categories_embed_layer = tf.keras.layers.Lambda(\n",
    "        lambda layer: tf.reduce_sum(layer, axis=1, keepdims=True))(\n",
    "            movie_categories_embed_layer)\n",
    "    #     movie_categories_embed_layer = tf.keras.layers.Reshape([1, 18 * embed_dim])(movie_categories_embed_layer)\n",
    "\n",
    "    return movie_categories_embed_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Movie Title地文本卷积网络实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_cnn_layer(movie_titles):\n",
    "    #从嵌入矩阵中得到电影名对应的各个单词的嵌入向量\n",
    "    movie_title_embed_layer = tf.keras.layers.Embedding(\n",
    "        movie_title_max,\n",
    "        embed_dim,\n",
    "        input_length=15,\n",
    "        name='movie_title_embed_layer')(movie_titles)\n",
    "    sp = movie_title_embed_layer.shape\n",
    "    movie_title_embed_layer_expand = tf.keras.layers.Reshape(\n",
    "        [sp[1], sp[2], 1])(movie_title_embed_layer)\n",
    "    #对文本嵌入层使用不同尺寸的卷积核做卷积和最大池化\n",
    "    pool_layer_lst = []\n",
    "    for window_size in window_sizes:\n",
    "        conv_layer = tf.keras.layers.Conv2D(\n",
    "            filter_num, (window_size, embed_dim), 1,\n",
    "            activation='relu')(movie_title_embed_layer_expand)\n",
    "        maxpool_layer = tf.keras.layers.MaxPooling2D(\n",
    "            pool_size=(sentences_size - window_size + 1, 1),\n",
    "            strides=1)(conv_layer)\n",
    "        pool_layer_lst.append(maxpool_layer)\n",
    "    #Dropout层\n",
    "    pool_layer = tf.keras.layers.concatenate(pool_layer_lst,\n",
    "                                             3,\n",
    "                                             name=\"pool_layer\")\n",
    "    max_num = len(window_sizes) * filter_num\n",
    "    pool_layer_flat = tf.keras.layers.Reshape(\n",
    "        [1, max_num], name=\"pool_layer_flat\")(pool_layer)\n",
    "\n",
    "    dropout_layer = tf.keras.layers.Dropout(\n",
    "        dropout_keep, name=\"dropout_layer\")(pool_layer_flat)\n",
    "    return pool_layer_flat, dropout_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将Movie地各个层一起做全连接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_feature_layer(movie_id_embed_layer, movie_categories_embed_layer,\n",
    "                            dropout_layer):\n",
    "    #第一层全连接\n",
    "    movie_id_fc_layer = tf.keras.layers.Dense(\n",
    "        embed_dim, name=\"movie_id_fc_layer\",\n",
    "        activation='relu')(movie_id_embed_layer)\n",
    "    movie_categories_fc_layer = tf.keras.layers.Dense(\n",
    "        embed_dim, name=\"movie_categories_fc_layer\",\n",
    "        activation='relu')(movie_categories_embed_layer)\n",
    "\n",
    "    #第二层全连接\n",
    "    movie_combine_layer = tf.keras.layers.concatenate(\n",
    "        [movie_id_fc_layer, movie_categories_fc_layer, dropout_layer], 2)\n",
    "    movie_combine_layer = tf.keras.layers.Dense(\n",
    "        200, activation='tanh')(movie_combine_layer)\n",
    "\n",
    "    movie_combine_layer_flat = tf.keras.layers.Reshape(\n",
    "        [200], name=\"movie_combine_layer_flat\")(movie_combine_layer)\n",
    "    return movie_combine_layer, movie_combine_layer_flat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建计算图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.ops import summary_ops_v2\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = './models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mv_network(object):\n",
    "    def __init__(self, batch_size=256):\n",
    "        self.batch_size = batch_size\n",
    "        self.best_loss = 9999\n",
    "        self.losses = {'train': [], 'test': []}\n",
    "\n",
    "        # 获取输入占位符\n",
    "        uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles = get_inputs(\n",
    "        )\n",
    "        # 获取User的4个嵌入向量\n",
    "        uid_embed_layer, gender_embed_layer, age_embed_layer, job_embed_layer = get_user_embedding(\n",
    "            uid, user_gender, user_age, user_job)\n",
    "        # 得到用户特征\n",
    "        user_combine_layer, user_combine_layer_flat = get_user_feature_layer(\n",
    "            uid_embed_layer, gender_embed_layer, age_embed_layer,\n",
    "            job_embed_layer)\n",
    "        # 获取电影ID的嵌入向量\n",
    "        movie_id_embed_layer = get_movie_id_embed_layer(movie_id)\n",
    "        # 获取电影类型的嵌入向量\n",
    "        movie_categories_embed_layer = get_movie_categories_layers(\n",
    "            movie_categories)\n",
    "        # 获取电影名的特征向量\n",
    "        pool_layer_flat, dropout_layer = get_movie_cnn_layer(movie_titles)\n",
    "        # 得到电影特征\n",
    "        movie_combine_layer, movie_combine_layer_flat = get_movie_feature_layer(\n",
    "            movie_id_embed_layer, movie_categories_embed_layer, dropout_layer)\n",
    "        # 计算出评分\n",
    "        # 将用户特征和电影特征做矩阵乘法得到一个预测评分的方案\n",
    "        inference = tf.keras.layers.Lambda(\n",
    "            lambda layer: tf.reduce_sum(layer[0] * layer[1], axis=1),\n",
    "            name=\"inference\")(\n",
    "                (user_combine_layer_flat, movie_combine_layer_flat))\n",
    "        inference = tf.keras.layers.Lambda(\n",
    "            lambda layer: tf.expand_dims(layer, axis=1))(inference)\n",
    "\n",
    "        # 将用户特征和电影特征作为输入，经过全连接，输出一个值的方案\n",
    "        #         inference_layer = tf.keras.layers.concatenate([user_combine_layer_flat, movie_combine_layer_flat],\n",
    "        #                                                       1)  # (?, 400)\n",
    "        # 你可以使用下面这个全连接层，试试效果\n",
    "        #inference_dense = tf.keras.layers.Dense(64, kernel_regularizer=tf.nn.l2_loss, activation='relu')(\n",
    "        #    inference_layer)\n",
    "        #         inference = tf.keras.layers.Dense(1, name=\"inference\")(inference_layer)  # inference_dense\n",
    "\n",
    "        self.model = tf.keras.Model(inputs=[\n",
    "            uid, user_gender, user_age, user_job, movie_id, movie_categories,\n",
    "            movie_titles\n",
    "        ],\n",
    "                                    outputs=[inference])\n",
    "\n",
    "        self.model.summary()\n",
    "\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "        # MSE损失，将计算值回归到评分\n",
    "        self.ComputeLoss = tf.keras.losses.MeanSquaredError()\n",
    "        self.ComputeMetrics = tf.keras.metrics.MeanAbsoluteError()\n",
    "\n",
    "        if tf.io.gfile.exists(MODEL_DIR):\n",
    "            #             print('Removing existing model dir: {}'.format(MODEL_DIR))\n",
    "            #             tf.io.gfile.rmtree(MODEL_DIR)\n",
    "            pass\n",
    "        else:\n",
    "            tf.io.gfile.makedirs(MODEL_DIR)\n",
    "\n",
    "        train_dir = os.path.join(MODEL_DIR, 'summaries', 'train')\n",
    "        test_dir = os.path.join(MODEL_DIR, 'summaries', 'eval')\n",
    "\n",
    "        #         self.train_summary_writer = summary_ops_v2.create_file_writer(train_dir, flush_millis=10000)\n",
    "        #         self.test_summary_writer = summary_ops_v2.create_file_writer(test_dir, flush_millis=10000, name='test')\n",
    "\n",
    "        checkpoint_dir = os.path.join(MODEL_DIR, 'checkpoints')\n",
    "        self.checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
    "        self.checkpoint = tf.train.Checkpoint(model=self.model,\n",
    "                                              optimizer=self.optimizer)\n",
    "\n",
    "        # Restore variables on creation if a checkpoint exists.\n",
    "        self.checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "    def compute_loss(self, labels, logits):\n",
    "        return tf.reduce_mean(tf.keras.losses.mse(labels, logits))\n",
    "\n",
    "    def compute_metrics(self, labels, logits):\n",
    "        return tf.keras.metrics.mae(labels, logits)  #\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, x, y):\n",
    "        # Record the operations used to compute the loss, so that the gradient\n",
    "        # of the loss with respect to the variables can be computed.\n",
    "        #         metrics = 0\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = self.model([x[0], x[1], x[2], x[3], x[4], x[5], x[6]],\n",
    "                                training=True)\n",
    "            loss = self.ComputeLoss(y, logits)\n",
    "            # loss = self.compute_loss(labels, logits)\n",
    "            self.ComputeMetrics(y, logits)\n",
    "            # metrics = self.compute_metrics(labels, logits)\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(grads, self.model.trainable_variables))\n",
    "        return loss, logits\n",
    "\n",
    "    def training(self, features, targets_values, epochs=5, log_freq=50):\n",
    "\n",
    "        for epoch_i in range(epochs):\n",
    "            # 将数据集分成训练集和测试集，随机种子不固定\n",
    "            train_X, test_X, train_y, test_y = train_test_split(features,\n",
    "                                                                targets_values,\n",
    "                                                                test_size=0.2,\n",
    "                                                                random_state=0)\n",
    "\n",
    "            train_batches = get_batches(train_X, train_y, self.batch_size)\n",
    "            batch_num = (len(train_X) // self.batch_size)\n",
    "\n",
    "            train_start = time.time()\n",
    "            #             with self.train_summary_writer.as_default():\n",
    "            if True:\n",
    "                start = time.time()\n",
    "                # Metrics are stateful. They accumulate values and return a cumulative\n",
    "                # result when you call .result(). Clear accumulated values with .reset_states()\n",
    "                avg_loss = tf.keras.metrics.Mean('loss', dtype=tf.float32)\n",
    "                #                 avg_mae = tf.keras.metrics.Mean('mae', dtype=tf.float32)\n",
    "\n",
    "                # Datasets can be iterated over like any other Python iterable.\n",
    "                for batch_i in range(batch_num):\n",
    "                    x, y = next(train_batches)\n",
    "                    categories = np.zeros([self.batch_size, 18])\n",
    "                    for i in range(self.batch_size):\n",
    "                        categories[i] = x.take(6, 1)[i]\n",
    "\n",
    "                    titles = np.zeros([self.batch_size, sentences_size])\n",
    "                    for i in range(self.batch_size):\n",
    "                        titles[i] = x.take(5, 1)[i]\n",
    "\n",
    "                    loss, logits = self.train_step([\n",
    "                        np.reshape(x.take(0, 1), [self.batch_size, 1]).astype(\n",
    "                            np.float32),\n",
    "                        np.reshape(x.take(2, 1), [self.batch_size, 1]).astype(\n",
    "                            np.float32),\n",
    "                        np.reshape(x.take(3, 1), [self.batch_size, 1]).astype(\n",
    "                            np.float32),\n",
    "                        np.reshape(x.take(4, 1), [self.batch_size, 1]).astype(\n",
    "                            np.float32),\n",
    "                        np.reshape(x.take(1, 1), [self.batch_size, 1]).astype(\n",
    "                            np.float32),\n",
    "                        categories.astype(np.float32),\n",
    "                        titles.astype(np.float32)\n",
    "                    ],\n",
    "                                                   np.reshape(\n",
    "                                                       y,\n",
    "                                                       [self.batch_size, 1\n",
    "                                                        ]).astype(np.float32))\n",
    "                    avg_loss(loss)\n",
    "                    #                     avg_mae(metrics)\n",
    "                    self.losses['train'].append(loss)\n",
    "\n",
    "                    if tf.equal(self.optimizer.iterations % log_freq, 0):\n",
    "                        #                         summary_ops_v2.scalar('loss', avg_loss.result(), step=self.optimizer.iterations)\n",
    "                        #                         summary_ops_v2.scalar('mae', self.ComputeMetrics.result(), step=self.optimizer.iterations)\n",
    "                        # summary_ops_v2.scalar('mae', avg_mae.result(), step=self.optimizer.iterations)\n",
    "\n",
    "                        rate = log_freq / (time.time() - start)\n",
    "                        print(\n",
    "                            'Step #{}\\tEpoch {:>3} Batch {:>4}/{}   Loss: {:0.6f} mae: {:0.6f} ({} steps/sec)'\n",
    "                            .format(self.optimizer.iterations.numpy(), epoch_i,\n",
    "                                    batch_i, batch_num, loss,\n",
    "                                    (self.ComputeMetrics.result()), rate))\n",
    "                        # print('Step #{}\\tLoss: {:0.6f} mae: {:0.6f} ({} steps/sec)'.format(\n",
    "                        #     self.optimizer.iterations.numpy(), loss, (avg_mae.result()), rate))\n",
    "                        avg_loss.reset_states()\n",
    "                        self.ComputeMetrics.reset_states()\n",
    "                        # avg_mae.reset_states()\n",
    "                        start = time.time()\n",
    "\n",
    "            train_end = time.time()\n",
    "            print('\\nTrain time for epoch #{} ({} total steps): {}'.format(\n",
    "                epoch_i + 1, self.optimizer.iterations.numpy(),\n",
    "                train_end - train_start))\n",
    "            #             with self.test_summary_writer.as_default():\n",
    "            self.testing((test_X, test_y), self.optimizer.iterations)\n",
    "            # self.checkpoint.save(self.checkpoint_prefix)\n",
    "        self.export_path = os.path.join(MODEL_DIR, '/export')\n",
    "        tf.saved_model.save(self.model, self.export_path)\n",
    "\n",
    "    def testing(self, test_dataset, step_num):\n",
    "        test_X, test_y = test_dataset\n",
    "        test_batches = get_batches(test_X, test_y, self.batch_size)\n",
    "        \"\"\"Perform an evaluation of `model` on the examples from `dataset`.\"\"\"\n",
    "        avg_loss = tf.keras.metrics.Mean('loss', dtype=tf.float32)\n",
    "        #         avg_mae = tf.keras.metrics.Mean('mae', dtype=tf.float32)\n",
    "\n",
    "        batch_num = (len(test_X) // self.batch_size)\n",
    "        for batch_i in range(batch_num):\n",
    "            x, y = next(test_batches)\n",
    "            categories = np.zeros([self.batch_size, 18])\n",
    "            for i in range(self.batch_size):\n",
    "                categories[i] = x.take(6, 1)[i]\n",
    "\n",
    "            titles = np.zeros([self.batch_size, sentences_size])\n",
    "            for i in range(self.batch_size):\n",
    "                titles[i] = x.take(5, 1)[i]\n",
    "\n",
    "            logits = self.model([\n",
    "                np.reshape(x.take(0, 1), [self.batch_size, 1]).astype(\n",
    "                    np.float32),\n",
    "                np.reshape(x.take(2, 1), [self.batch_size, 1]).astype(\n",
    "                    np.float32),\n",
    "                np.reshape(x.take(3, 1), [self.batch_size, 1]).astype(\n",
    "                    np.float32),\n",
    "                np.reshape(x.take(4, 1), [self.batch_size, 1]).astype(\n",
    "                    np.float32),\n",
    "                np.reshape(x.take(1, 1), [self.batch_size, 1]).astype(\n",
    "                    np.float32),\n",
    "                categories.astype(np.float32),\n",
    "                titles.astype(np.float32)\n",
    "            ],\n",
    "                                training=False)\n",
    "            test_loss = self.ComputeLoss(\n",
    "                np.reshape(y, [self.batch_size, 1]).astype(np.float32), logits)\n",
    "            avg_loss(test_loss)\n",
    "            # 保存测试损失\n",
    "            self.losses['test'].append(test_loss)\n",
    "            self.ComputeMetrics(\n",
    "                np.reshape(y, [self.batch_size, 1]).astype(np.float32), logits)\n",
    "            # avg_loss(self.compute_loss(labels, logits))\n",
    "            # avg_mae(self.compute_metrics(labels, logits))\n",
    "\n",
    "        print('Model test set loss: {:0.6f} mae: {:0.6f}'.format(\n",
    "            avg_loss.result(), self.ComputeMetrics.result()))\n",
    "        # print('Model test set loss: {:0.6f} mae: {:0.6f}'.format(avg_loss.result(), avg_mae.result()))\n",
    "        #         summary_ops_v2.scalar('loss', avg_loss.result(), step=step_num)\n",
    "        #         summary_ops_v2.scalar('mae', self.ComputeMetrics.result(), step=step_num)\n",
    "        # summary_ops_v2.scalar('mae', avg_mae.result(), step=step_num)\n",
    "\n",
    "        if avg_loss.result() < self.best_loss:\n",
    "            self.best_loss = avg_loss.result()\n",
    "            print(\"best loss = {}\".format(self.best_loss))\n",
    "            self.checkpoint.save(self.checkpoint_prefix)\n",
    "\n",
    "    def forward(self, xs):\n",
    "        predictions = self.model(xs)\n",
    "        # logits = tf.nn.softmax(predictions)\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取得batch\n",
    "def get_batches(Xs, ys, batch_size):\n",
    "    for start in range(0, len(Xs), batch_size):\n",
    "        end = min(start + batch_size, len(Xs))\n",
    "        yield Xs[start:end], ys[start:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练网络\n",
    "将用户特征和电影特征作为输入， 经过全连接，输出一个值的训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "movie_titles (InputLayer)       [(None, 15)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "movie_title_embed_layer (Embedd (None, 15, 32)       166880      movie_titles[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 15, 32, 1)    0           movie_title_embed_layer[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 14, 1, 8)     520         reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 13, 1, 8)     776         reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 12, 1, 8)     1032        reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 11, 1, 8)     1288        reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "movie_categories (InputLayer)   [(None, 18)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 1, 1, 8)      0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 1, 1, 8)      0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 1, 1, 8)      0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 1, 1, 8)      0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "uid (InputLayer)                [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "user_gender (InputLayer)        [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "user_age (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "user_job (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "movie_id (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "movie_categories_embed_layer (E (None, 18, 32)       608         movie_categories[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "pool_layer (Concatenate)        (None, 1, 1, 32)     0           max_pooling2d[0][0]              \n",
      "                                                                 max_pooling2d_1[0][0]            \n",
      "                                                                 max_pooling2d_2[0][0]            \n",
      "                                                                 max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "uid_embed_layer (Embedding)     (None, 1, 32)        193312      uid[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "gender_embed_layer (Embedding)  (None, 1, 16)        32          user_gender[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "age_embed_layer (Embedding)     (None, 1, 16)        112         user_age[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "job_embed_layer (Embedding)     (None, 1, 16)        336         user_job[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "movie_id_embed_layer (Embedding (None, 1, 32)        126496      movie_id[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 1, 32)        0           movie_categories_embed_layer[0][0\n",
      "__________________________________________________________________________________________________\n",
      "pool_layer_flat (Reshape)       (None, 1, 32)        0           pool_layer[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "uid_fc_layer (Dense)            (None, 1, 32)        1056        uid_embed_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "gender_fc_layer (Dense)         (None, 1, 32)        544         gender_embed_layer[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "age_fc_layer (Dense)            (None, 1, 32)        544         age_embed_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "job_fc_layer (Dense)            (None, 1, 32)        544         job_embed_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "movie_id_fc_layer (Dense)       (None, 1, 32)        1056        movie_id_embed_layer[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "movie_categories_fc_layer (Dens (None, 1, 32)        1056        lambda[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_layer (Dropout)         (None, 1, 32)        0           pool_layer_flat[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 1, 128)       0           uid_fc_layer[0][0]               \n",
      "                                                                 gender_fc_layer[0][0]            \n",
      "                                                                 age_fc_layer[0][0]               \n",
      "                                                                 job_fc_layer[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1, 96)        0           movie_id_fc_layer[0][0]          \n",
      "                                                                 movie_categories_fc_layer[0][0]  \n",
      "                                                                 dropout_layer[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1, 200)       25800       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1, 200)       19400       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "user_combine_layer_flat (Reshap (None, 200)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "movie_combine_layer_flat (Resha (None, 200)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "inference (Lambda)              (None,)              0           user_combine_layer_flat[0][0]    \n",
      "                                                                 movie_combine_layer_flat[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 1)            0           inference[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 541,392\n",
      "Trainable params: 541,392\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #18800\tEpoch   0 Batch   49/3125   Loss: 0.763133 mae: 0.675904 (15.141735359699895 steps/sec)\n",
      "Step #18850\tEpoch   0 Batch   99/3125   Loss: 0.800664 mae: 0.643598 (44.4028984077305 steps/sec)\n",
      "Step #18900\tEpoch   0 Batch  149/3125   Loss: 0.632010 mae: 0.646578 (46.06202613857976 steps/sec)\n",
      "Step #18950\tEpoch   0 Batch  199/3125   Loss: 0.688822 mae: 0.642744 (44.96157104110116 steps/sec)\n",
      "Step #19000\tEpoch   0 Batch  249/3125   Loss: 0.760208 mae: 0.644846 (46.199023392150394 steps/sec)\n",
      "Step #19050\tEpoch   0 Batch  299/3125   Loss: 0.641236 mae: 0.641642 (45.02257513469834 steps/sec)\n",
      "Step #19100\tEpoch   0 Batch  349/3125   Loss: 0.689330 mae: 0.641615 (45.19771868280505 steps/sec)\n",
      "Step #19150\tEpoch   0 Batch  399/3125   Loss: 0.728691 mae: 0.642189 (46.28016737332412 steps/sec)\n",
      "Step #19200\tEpoch   0 Batch  449/3125   Loss: 0.648716 mae: 0.648071 (44.014041497384746 steps/sec)\n",
      "Step #19250\tEpoch   0 Batch  499/3125   Loss: 0.619456 mae: 0.632197 (43.935423588909885 steps/sec)\n",
      "Step #19300\tEpoch   0 Batch  549/3125   Loss: 0.656783 mae: 0.639300 (42.06968658449396 steps/sec)\n",
      "Step #19350\tEpoch   0 Batch  599/3125   Loss: 0.547306 mae: 0.647801 (41.847276692017985 steps/sec)\n",
      "Step #19400\tEpoch   0 Batch  649/3125   Loss: 0.622290 mae: 0.633210 (43.01499077613766 steps/sec)\n",
      "Step #19450\tEpoch   0 Batch  699/3125   Loss: 0.630532 mae: 0.636819 (38.13106936897018 steps/sec)\n",
      "Step #19500\tEpoch   0 Batch  749/3125   Loss: 0.653405 mae: 0.629667 (38.22374709196146 steps/sec)\n",
      "Step #19550\tEpoch   0 Batch  799/3125   Loss: 0.659138 mae: 0.639462 (43.03293514850622 steps/sec)\n",
      "Step #19600\tEpoch   0 Batch  849/3125   Loss: 0.708141 mae: 0.635037 (42.10479821377769 steps/sec)\n",
      "Step #19650\tEpoch   0 Batch  899/3125   Loss: 0.687295 mae: 0.636505 (43.602453453024516 steps/sec)\n",
      "Step #19700\tEpoch   0 Batch  949/3125   Loss: 0.588099 mae: 0.642120 (45.1223713067477 steps/sec)\n",
      "Step #19750\tEpoch   0 Batch  999/3125   Loss: 0.722298 mae: 0.626368 (44.91995745229555 steps/sec)\n",
      "Step #19800\tEpoch   0 Batch 1049/3125   Loss: 0.711247 mae: 0.638103 (44.542363046408845 steps/sec)\n",
      "Step #19850\tEpoch   0 Batch 1099/3125   Loss: 0.630279 mae: 0.631039 (45.24392843304686 steps/sec)\n",
      "Step #19900\tEpoch   0 Batch 1149/3125   Loss: 0.588963 mae: 0.641302 (44.31138610210826 steps/sec)\n",
      "Step #19950\tEpoch   0 Batch 1199/3125   Loss: 0.750166 mae: 0.629035 (45.323649428286956 steps/sec)\n",
      "Step #20000\tEpoch   0 Batch 1249/3125   Loss: 0.744337 mae: 0.638298 (45.11845910438282 steps/sec)\n",
      "Step #20050\tEpoch   0 Batch 1299/3125   Loss: 0.607661 mae: 0.644598 (44.7609284412779 steps/sec)\n",
      "Step #20100\tEpoch   0 Batch 1349/3125   Loss: 0.642484 mae: 0.622217 (45.02872332316664 steps/sec)\n",
      "Step #20150\tEpoch   0 Batch 1399/3125   Loss: 0.665766 mae: 0.628981 (45.26717135436813 steps/sec)\n",
      "Step #20200\tEpoch   0 Batch 1449/3125   Loss: 0.639546 mae: 0.628313 (44.84034508954697 steps/sec)\n",
      "Step #20250\tEpoch   0 Batch 1499/3125   Loss: 0.732605 mae: 0.629057 (45.38048464662482 steps/sec)\n",
      "Step #20300\tEpoch   0 Batch 1549/3125   Loss: 0.746087 mae: 0.638422 (44.860651140344835 steps/sec)\n",
      "Step #20350\tEpoch   0 Batch 1599/3125   Loss: 0.604326 mae: 0.628881 (45.04614263706825 steps/sec)\n",
      "Step #20400\tEpoch   0 Batch 1649/3125   Loss: 0.707040 mae: 0.636776 (44.68038370786218 steps/sec)\n",
      "Step #20450\tEpoch   0 Batch 1699/3125   Loss: 0.674191 mae: 0.629935 (45.20284302472828 steps/sec)\n",
      "Step #20500\tEpoch   0 Batch 1749/3125   Loss: 0.698467 mae: 0.629267 (45.06496986538863 steps/sec)\n",
      "Step #20550\tEpoch   0 Batch 1799/3125   Loss: 0.773932 mae: 0.628348 (45.6159843181133 steps/sec)\n",
      "Step #20600\tEpoch   0 Batch 1849/3125   Loss: 0.607457 mae: 0.632908 (44.94090388743859 steps/sec)\n",
      "Step #20650\tEpoch   0 Batch 1899/3125   Loss: 0.736202 mae: 0.634296 (45.13188768247015 steps/sec)\n",
      "Step #20700\tEpoch   0 Batch 1949/3125   Loss: 0.600540 mae: 0.630633 (44.28226854900881 steps/sec)\n",
      "Step #20750\tEpoch   0 Batch 1999/3125   Loss: 0.698686 mae: 0.633569 (44.81090187102404 steps/sec)\n",
      "Step #20800\tEpoch   0 Batch 2049/3125   Loss: 0.631773 mae: 0.636285 (44.82419587330954 steps/sec)\n",
      "Step #20850\tEpoch   0 Batch 2099/3125   Loss: 0.677780 mae: 0.627810 (39.29633000378694 steps/sec)\n",
      "Step #20900\tEpoch   0 Batch 2149/3125   Loss: 0.613229 mae: 0.623905 (39.83340699858513 steps/sec)\n",
      "Step #20950\tEpoch   0 Batch 2199/3125   Loss: 0.631897 mae: 0.632328 (44.46336585969846 steps/sec)\n",
      "Step #21000\tEpoch   0 Batch 2249/3125   Loss: 0.554518 mae: 0.633524 (45.534649204515006 steps/sec)\n",
      "Step #21050\tEpoch   0 Batch 2299/3125   Loss: 0.647007 mae: 0.643940 (45.0242860185357 steps/sec)\n",
      "Step #21100\tEpoch   0 Batch 2349/3125   Loss: 0.632153 mae: 0.629731 (45.45914089534794 steps/sec)\n",
      "Step #21150\tEpoch   0 Batch 2399/3125   Loss: 0.555441 mae: 0.628194 (44.8751749566313 steps/sec)\n",
      "Step #21200\tEpoch   0 Batch 2449/3125   Loss: 0.574358 mae: 0.627631 (44.78469136572304 steps/sec)\n",
      "Step #21250\tEpoch   0 Batch 2499/3125   Loss: 0.791457 mae: 0.639051 (44.67525340770574 steps/sec)\n",
      "Step #21300\tEpoch   0 Batch 2549/3125   Loss: 0.492336 mae: 0.636211 (43.11821841937642 steps/sec)\n",
      "Step #21350\tEpoch   0 Batch 2599/3125   Loss: 0.662373 mae: 0.636072 (44.86418283578418 steps/sec)\n",
      "Step #21400\tEpoch   0 Batch 2649/3125   Loss: 0.737684 mae: 0.632157 (44.52931130654648 steps/sec)\n",
      "Step #21450\tEpoch   0 Batch 2699/3125   Loss: 0.627469 mae: 0.632623 (45.65780464360864 steps/sec)\n",
      "Step #21500\tEpoch   0 Batch 2749/3125   Loss: 0.715322 mae: 0.630153 (45.30983240682126 steps/sec)\n",
      "Step #21550\tEpoch   0 Batch 2799/3125   Loss: 0.651249 mae: 0.638505 (45.716627591265855 steps/sec)\n",
      "Step #21600\tEpoch   0 Batch 2849/3125   Loss: 0.624327 mae: 0.629211 (44.90600080126884 steps/sec)\n",
      "Step #21650\tEpoch   0 Batch 2899/3125   Loss: 0.718721 mae: 0.629302 (45.03062805550498 steps/sec)\n",
      "Step #21700\tEpoch   0 Batch 2949/3125   Loss: 0.749628 mae: 0.633520 (45.06615132662598 steps/sec)\n",
      "Step #21750\tEpoch   0 Batch 2999/3125   Loss: 0.634643 mae: 0.629004 (44.85762852746951 steps/sec)\n",
      "Step #21800\tEpoch   0 Batch 3049/3125   Loss: 0.673290 mae: 0.619364 (44.97938219065672 steps/sec)\n",
      "Step #21850\tEpoch   0 Batch 3099/3125   Loss: 0.628306 mae: 0.625507 (43.975591855564176 steps/sec)\n",
      "\n",
      "Train time for epoch #1 (21875 total steps): 72.866455078125\n",
      "Model test set loss: 0.787287 mae: 0.694293\n",
      "best loss = 0.7872871160507202\n",
      "Step #21900\tEpoch   1 Batch   24/3125   Loss: 0.609793 mae: 0.692453 (86.14047972292461 steps/sec)\n",
      "Step #21950\tEpoch   1 Batch   74/3125   Loss: 0.663495 mae: 0.628299 (46.35364494045856 steps/sec)\n",
      "Step #22000\tEpoch   1 Batch  124/3125   Loss: 0.612528 mae: 0.631154 (45.23831660082743 steps/sec)\n",
      "Step #22050\tEpoch   1 Batch  174/3125   Loss: 0.640396 mae: 0.631068 (45.99236411030964 steps/sec)\n",
      "Step #22100\tEpoch   1 Batch  224/3125   Loss: 0.597548 mae: 0.624831 (44.76149211234598 steps/sec)\n",
      "Step #22150\tEpoch   1 Batch  274/3125   Loss: 0.584429 mae: 0.629141 (44.98294225462533 steps/sec)\n",
      "Step #22200\tEpoch   1 Batch  324/3125   Loss: 0.705390 mae: 0.625993 (44.306012568305434 steps/sec)\n",
      "Step #22250\tEpoch   1 Batch  374/3125   Loss: 0.581931 mae: 0.630010 (45.346503696326295 steps/sec)\n",
      "Step #22300\tEpoch   1 Batch  424/3125   Loss: 0.714250 mae: 0.632427 (45.51145679274908 steps/sec)\n",
      "Step #22350\tEpoch   1 Batch  474/3125   Loss: 0.681139 mae: 0.624644 (45.4659904569532 steps/sec)\n",
      "Step #22400\tEpoch   1 Batch  524/3125   Loss: 0.721956 mae: 0.622222 (43.218112620258 steps/sec)\n",
      "Step #22450\tEpoch   1 Batch  574/3125   Loss: 0.707062 mae: 0.634292 (44.410702944849376 steps/sec)\n",
      "Step #22500\tEpoch   1 Batch  624/3125   Loss: 0.424980 mae: 0.625206 (45.632947515904554 steps/sec)\n",
      "Step #22550\tEpoch   1 Batch  674/3125   Loss: 0.644515 mae: 0.625012 (44.66591909388215 steps/sec)\n",
      "Step #22600\tEpoch   1 Batch  724/3125   Loss: 0.559798 mae: 0.621314 (45.46177206551657 steps/sec)\n",
      "Step #22650\tEpoch   1 Batch  774/3125   Loss: 0.522859 mae: 0.624440 (40.36413237936917 steps/sec)\n",
      "Step #22700\tEpoch   1 Batch  824/3125   Loss: 0.658959 mae: 0.627595 (39.303400069305404 steps/sec)\n",
      "Step #22750\tEpoch   1 Batch  874/3125   Loss: 0.611395 mae: 0.621795 (45.55409487945657 steps/sec)\n",
      "Step #22800\tEpoch   1 Batch  924/3125   Loss: 0.766261 mae: 0.635363 (45.364611375460456 steps/sec)\n",
      "Step #22850\tEpoch   1 Batch  974/3125   Loss: 0.603483 mae: 0.626996 (46.365440639845346 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #22900\tEpoch   1 Batch 1024/3125   Loss: 0.631318 mae: 0.617904 (44.92148734396162 steps/sec)\n",
      "Step #22950\tEpoch   1 Batch 1074/3125   Loss: 0.644288 mae: 0.622262 (44.85660189132698 steps/sec)\n",
      "Step #23000\tEpoch   1 Batch 1124/3125   Loss: 0.620328 mae: 0.632210 (43.53297752495284 steps/sec)\n",
      "Step #23050\tEpoch   1 Batch 1174/3125   Loss: 0.533237 mae: 0.626361 (45.65477304887341 steps/sec)\n",
      "Step #23100\tEpoch   1 Batch 1224/3125   Loss: 0.641421 mae: 0.618252 (45.80639735971579 steps/sec)\n",
      "Step #23150\tEpoch   1 Batch 1274/3125   Loss: 0.667527 mae: 0.632826 (44.40381976447143 steps/sec)\n",
      "Step #23200\tEpoch   1 Batch 1324/3125   Loss: 0.538211 mae: 0.629792 (45.005270621521305 steps/sec)\n",
      "Step #23250\tEpoch   1 Batch 1374/3125   Loss: 0.600053 mae: 0.616821 (44.800697276477386 steps/sec)\n",
      "Step #23300\tEpoch   1 Batch 1424/3125   Loss: 0.646433 mae: 0.616096 (44.77297880771175 steps/sec)\n",
      "Step #23350\tEpoch   1 Batch 1474/3125   Loss: 0.523652 mae: 0.622667 (45.42986382210114 steps/sec)\n",
      "Step #23400\tEpoch   1 Batch 1524/3125   Loss: 0.579924 mae: 0.618300 (45.55264033030113 steps/sec)\n",
      "Step #23450\tEpoch   1 Batch 1574/3125   Loss: 0.652666 mae: 0.627998 (45.57378495556932 steps/sec)\n",
      "Step #23500\tEpoch   1 Batch 1624/3125   Loss: 0.709574 mae: 0.618893 (44.46086783782987 steps/sec)\n",
      "Step #23550\tEpoch   1 Batch 1674/3125   Loss: 0.514207 mae: 0.628070 (45.613910690651565 steps/sec)\n",
      "Step #23600\tEpoch   1 Batch 1724/3125   Loss: 0.720909 mae: 0.622108 (45.02512700910335 steps/sec)\n",
      "Step #23650\tEpoch   1 Batch 1774/3125   Loss: 0.568278 mae: 0.620230 (45.687883959007195 steps/sec)\n",
      "Step #23700\tEpoch   1 Batch 1824/3125   Loss: 0.563817 mae: 0.621068 (45.39544532907797 steps/sec)\n",
      "Step #23750\tEpoch   1 Batch 1874/3125   Loss: 0.658959 mae: 0.628995 (46.24730436569815 steps/sec)\n",
      "Step #23800\tEpoch   1 Batch 1924/3125   Loss: 0.807605 mae: 0.629245 (43.79583140683232 steps/sec)\n",
      "Step #23850\tEpoch   1 Batch 1974/3125   Loss: 0.585608 mae: 0.621929 (44.57461849697714 steps/sec)\n",
      "Step #23900\tEpoch   1 Batch 2024/3125   Loss: 0.734126 mae: 0.624594 (45.525713325584896 steps/sec)\n",
      "Step #23950\tEpoch   1 Batch 2074/3125   Loss: 0.703115 mae: 0.618384 (44.67887971301919 steps/sec)\n",
      "Step #24000\tEpoch   1 Batch 2124/3125   Loss: 0.551623 mae: 0.623706 (44.79375009077713 steps/sec)\n",
      "Step #24050\tEpoch   1 Batch 2174/3125   Loss: 0.602354 mae: 0.620231 (42.41132035254054 steps/sec)\n",
      "Step #24100\tEpoch   1 Batch 2224/3125   Loss: 0.615729 mae: 0.628866 (36.73964133707185 steps/sec)\n",
      "Step #24150\tEpoch   1 Batch 2274/3125   Loss: 0.673081 mae: 0.626667 (43.4401617340006 steps/sec)\n",
      "Step #24200\tEpoch   1 Batch 2324/3125   Loss: 0.523631 mae: 0.630882 (35.851810265525216 steps/sec)\n",
      "Step #24250\tEpoch   1 Batch 2374/3125   Loss: 0.586809 mae: 0.619353 (41.12582059381616 steps/sec)\n",
      "Step #24300\tEpoch   1 Batch 2424/3125   Loss: 0.593359 mae: 0.623217 (41.092620010361564 steps/sec)\n",
      "Step #24350\tEpoch   1 Batch 2474/3125   Loss: 0.700221 mae: 0.622178 (44.76085201239553 steps/sec)\n",
      "Step #24400\tEpoch   1 Batch 2524/3125   Loss: 0.713022 mae: 0.626059 (45.42961779065591 steps/sec)\n",
      "Step #24450\tEpoch   1 Batch 2574/3125   Loss: 0.660399 mae: 0.630633 (44.56164253741143 steps/sec)\n",
      "Step #24500\tEpoch   1 Batch 2624/3125   Loss: 0.755305 mae: 0.628086 (45.42910605378572 steps/sec)\n",
      "Step #24550\tEpoch   1 Batch 2674/3125   Loss: 0.739559 mae: 0.627092 (45.27323015440572 steps/sec)\n",
      "Step #24600\tEpoch   1 Batch 2724/3125   Loss: 0.624868 mae: 0.620242 (46.24822226294653 steps/sec)\n",
      "Step #24650\tEpoch   1 Batch 2774/3125   Loss: 0.583062 mae: 0.629360 (43.44797352786891 steps/sec)\n",
      "Step #24700\tEpoch   1 Batch 2824/3125   Loss: 0.631087 mae: 0.630644 (44.876749814952994 steps/sec)\n",
      "Step #24750\tEpoch   1 Batch 2874/3125   Loss: 0.679287 mae: 0.618242 (44.434763815730925 steps/sec)\n",
      "Step #24800\tEpoch   1 Batch 2924/3125   Loss: 0.652099 mae: 0.624062 (44.72233643090015 steps/sec)\n",
      "Step #24850\tEpoch   1 Batch 2974/3125   Loss: 0.589381 mae: 0.623282 (44.74156206924434 steps/sec)\n",
      "Step #24900\tEpoch   1 Batch 3024/3125   Loss: 0.579640 mae: 0.617220 (44.37111622189451 steps/sec)\n",
      "Step #24950\tEpoch   1 Batch 3074/3125   Loss: 0.625207 mae: 0.616733 (44.56333750743465 steps/sec)\n",
      "Step #25000\tEpoch   1 Batch 3124/3125   Loss: 0.762946 mae: 0.615793 (44.22844773407322 steps/sec)\n",
      "\n",
      "Train time for epoch #2 (25000 total steps): 70.67844080924988\n",
      "Model test set loss: 0.792960 mae: 0.698860\n",
      "Step #25050\tEpoch   2 Batch   49/3125   Loss: 0.676332 mae: 0.694503 (57.89111627624511 steps/sec)\n",
      "Step #25100\tEpoch   2 Batch   99/3125   Loss: 0.729866 mae: 0.615895 (59.32987281638325 steps/sec)\n",
      "Step #25150\tEpoch   2 Batch  149/3125   Loss: 0.579831 mae: 0.623600 (59.47062783613499 steps/sec)\n",
      "Step #25200\tEpoch   2 Batch  199/3125   Loss: 0.641245 mae: 0.617933 (59.40016694287489 steps/sec)\n",
      "Step #25250\tEpoch   2 Batch  249/3125   Loss: 0.710834 mae: 0.618549 (59.18981591241106 steps/sec)\n",
      "Step #25300\tEpoch   2 Batch  299/3125   Loss: 0.627418 mae: 0.622655 (59.89692876980568 steps/sec)\n",
      "Step #25350\tEpoch   2 Batch  349/3125   Loss: 0.642885 mae: 0.620490 (59.54122228105565 steps/sec)\n",
      "Step #25400\tEpoch   2 Batch  399/3125   Loss: 0.687329 mae: 0.619757 (59.89692876980568 steps/sec)\n",
      "Step #25450\tEpoch   2 Batch  449/3125   Loss: 0.628125 mae: 0.629212 (59.05026905767726 steps/sec)\n",
      "Step #25500\tEpoch   2 Batch  499/3125   Loss: 0.586438 mae: 0.611408 (59.54122228105565 steps/sec)\n",
      "Step #25550\tEpoch   2 Batch  549/3125   Loss: 0.605320 mae: 0.619462 (58.84240105611228 steps/sec)\n",
      "Step #25600\tEpoch   2 Batch  599/3125   Loss: 0.534858 mae: 0.629580 (58.91154414028527 steps/sec)\n",
      "Step #25650\tEpoch   2 Batch  649/3125   Loss: 0.609491 mae: 0.617985 (58.3628419558623 steps/sec)\n",
      "Step #25700\tEpoch   2 Batch  699/3125   Loss: 0.589242 mae: 0.614601 (59.54117156721901 steps/sec)\n",
      "Step #25750\tEpoch   2 Batch  749/3125   Loss: 0.605778 mae: 0.609657 (59.259677884687385 steps/sec)\n",
      "Step #25800\tEpoch   2 Batch  799/3125   Loss: 0.635582 mae: 0.624196 (59.32988960121718 steps/sec)\n",
      "Step #25850\tEpoch   2 Batch  849/3125   Loss: 0.687126 mae: 0.620833 (58.02516787204731 steps/sec)\n",
      "Step #25900\tEpoch   2 Batch  899/3125   Loss: 0.660124 mae: 0.619953 (59.82542056618645 steps/sec)\n",
      "Step #25950\tEpoch   2 Batch  949/3125   Loss: 0.576979 mae: 0.624488 (59.189749089780136 steps/sec)\n",
      "Step #26000\tEpoch   2 Batch  999/3125   Loss: 0.725597 mae: 0.614519 (59.89691166260005 steps/sec)\n",
      "Step #26050\tEpoch   2 Batch 1049/3125   Loss: 0.688004 mae: 0.618821 (58.43094336426392 steps/sec)\n",
      "Step #26100\tEpoch   2 Batch 1099/3125   Loss: 0.590980 mae: 0.611923 (58.77338714197634 steps/sec)\n",
      "Step #26150\tEpoch   2 Batch 1149/3125   Loss: 0.578125 mae: 0.625186 (58.29499633354663 steps/sec)\n",
      "Step #26200\tEpoch   2 Batch 1199/3125   Loss: 0.673588 mae: 0.612137 (59.050368819835654 steps/sec)\n",
      "Step #26250\tEpoch   2 Batch 1249/3125   Loss: 0.701248 mae: 0.621188 (42.703304002477715 steps/sec)\n",
      "Step #26300\tEpoch   2 Batch 1299/3125   Loss: 0.572773 mae: 0.626756 (52.005965497522396 steps/sec)\n",
      "Step #26350\tEpoch   2 Batch 1349/3125   Loss: 0.605751 mae: 0.609784 (59.259711374977144 steps/sec)\n",
      "Step #26400\tEpoch   2 Batch 1399/3125   Loss: 0.621331 mae: 0.613938 (58.49908966169781 steps/sec)\n",
      "Step #26450\tEpoch   2 Batch 1449/3125   Loss: 0.626212 mae: 0.612747 (58.980849909538314 steps/sec)\n",
      "Step #26500\tEpoch   2 Batch 1499/3125   Loss: 0.716353 mae: 0.615760 (58.911494493424556 steps/sec)\n",
      "Step #26550\tEpoch   2 Batch 1549/3125   Loss: 0.701129 mae: 0.618090 (58.02508759852759 steps/sec)\n",
      "Step #26600\tEpoch   2 Batch 1599/3125   Loss: 0.576744 mae: 0.612759 (59.4001164690394 steps/sec)\n",
      "Step #26650\tEpoch   2 Batch 1649/3125   Loss: 0.677698 mae: 0.620638 (58.295174582366336 steps/sec)\n",
      "Step #26700\tEpoch   2 Batch 1699/3125   Loss: 0.652778 mae: 0.616830 (58.22727593285996 steps/sec)\n",
      "Step #26750\tEpoch   2 Batch 1749/3125   Loss: 0.677653 mae: 0.615792 (58.29506115117313 steps/sec)\n",
      "Step #26800\tEpoch   2 Batch 1799/3125   Loss: 0.750970 mae: 0.613615 (60.09523989644997 steps/sec)\n",
      "Step #26850\tEpoch   2 Batch 1849/3125   Loss: 0.575015 mae: 0.615984 (56.970084859443226 steps/sec)\n",
      "Step #26900\tEpoch   2 Batch 1899/3125   Loss: 0.706665 mae: 0.620140 (59.259728120136224 steps/sec)\n",
      "Step #26950\tEpoch   2 Batch 1949/3125   Loss: 0.598146 mae: 0.619520 (57.36117250101749 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #27000\tEpoch   2 Batch 1999/3125   Loss: 0.665007 mae: 0.619339 (58.02510365321377 steps/sec)\n",
      "Step #27050\tEpoch   2 Batch 2049/3125   Loss: 0.600302 mae: 0.619831 (59.18973238414598 steps/sec)\n",
      "Step #27100\tEpoch   2 Batch 2099/3125   Loss: 0.630337 mae: 0.611807 (58.22732443318545 steps/sec)\n",
      "Step #27150\tEpoch   2 Batch 2149/3125   Loss: 0.594576 mae: 0.611183 (58.77346949928465 steps/sec)\n",
      "Step #27200\tEpoch   2 Batch 2199/3125   Loss: 0.599276 mae: 0.619994 (57.89116421817949 steps/sec)\n",
      "Step #27250\tEpoch   2 Batch 2249/3125   Loss: 0.508298 mae: 0.618357 (58.704551358420595 steps/sec)\n",
      "Step #27300\tEpoch   2 Batch 2299/3125   Loss: 0.653273 mae: 0.628758 (56.97013128802657 steps/sec)\n",
      "Step #27350\tEpoch   2 Batch 2349/3125   Loss: 0.605656 mae: 0.615799 (58.91156068925744 steps/sec)\n",
      "Step #27400\tEpoch   2 Batch 2399/3125   Loss: 0.545579 mae: 0.614135 (58.635925811497906 steps/sec)\n",
      "Step #27450\tEpoch   2 Batch 2449/3125   Loss: 0.538630 mae: 0.613800 (59.05033556574541 steps/sec)\n",
      "Step #27500\tEpoch   2 Batch 2499/3125   Loss: 0.721503 mae: 0.622450 (60.04037329407659 steps/sec)\n",
      "Step #27550\tEpoch   2 Batch 2549/3125   Loss: 0.479273 mae: 0.620856 (57.95804053562945 steps/sec)\n",
      "Step #27600\tEpoch   2 Batch 2599/3125   Loss: 0.668924 mae: 0.620863 (59.40016694287489 steps/sec)\n",
      "Step #27650\tEpoch   2 Batch 2649/3125   Loss: 0.727457 mae: 0.617368 (59.470526648918494 steps/sec)\n",
      "Step #27700\tEpoch   2 Batch 2699/3125   Loss: 0.599626 mae: 0.620676 (58.15977720623676 steps/sec)\n",
      "Step #27750\tEpoch   2 Batch 2749/3125   Loss: 0.690249 mae: 0.616834 (58.159696559841215 steps/sec)\n",
      "Step #27800\tEpoch   2 Batch 2799/3125   Loss: 0.613959 mae: 0.627860 (59.11998513791267 steps/sec)\n",
      "Step #27850\tEpoch   2 Batch 2849/3125   Loss: 0.589523 mae: 0.615331 (59.189749089780136 steps/sec)\n",
      "Step #27900\tEpoch   2 Batch 2899/3125   Loss: 0.675143 mae: 0.612771 (59.754065874011545 steps/sec)\n",
      "Step #27950\tEpoch   2 Batch 2949/3125   Loss: 0.734121 mae: 0.621704 (59.47059410702457 steps/sec)\n",
      "Step #28000\tEpoch   2 Batch 2999/3125   Loss: 0.619123 mae: 0.614982 (58.430894524137564 steps/sec)\n",
      "Step #28050\tEpoch   2 Batch 3049/3125   Loss: 0.636042 mae: 0.606835 (56.64825979272616 steps/sec)\n",
      "Step #28100\tEpoch   2 Batch 3099/3125   Loss: 0.606233 mae: 0.611350 (50.74261037169595 steps/sec)\n",
      "\n",
      "Train time for epoch #3 (28125 total steps): 53.861175298690796\n",
      "Model test set loss: 0.798713 mae: 0.698383\n",
      "Step #28150\tEpoch   3 Batch   24/3125   Loss: 0.578002 mae: 0.695849 (107.81435477245725 steps/sec)\n",
      "Step #28200\tEpoch   3 Batch   74/3125   Loss: 0.646578 mae: 0.614838 (59.61201841272404 steps/sec)\n",
      "Step #28250\tEpoch   3 Batch  124/3125   Loss: 0.600636 mae: 0.617135 (59.32987281638325 steps/sec)\n",
      "Step #28300\tEpoch   3 Batch  174/3125   Loss: 0.600811 mae: 0.614874 (59.32982246193845 steps/sec)\n",
      "Step #28350\tEpoch   3 Batch  224/3125   Loss: 0.562701 mae: 0.609313 (58.704617090007225 steps/sec)\n",
      "Step #28400\tEpoch   3 Batch  274/3125   Loss: 0.565607 mae: 0.615952 (58.70460065709676 steps/sec)\n",
      "Step #28450\tEpoch   3 Batch  324/3125   Loss: 0.686805 mae: 0.612655 (57.03495741997667 steps/sec)\n",
      "Step #28500\tEpoch   3 Batch  374/3125   Loss: 0.540408 mae: 0.614502 (58.70460065709676 steps/sec)\n",
      "Step #28550\tEpoch   3 Batch  424/3125   Loss: 0.705520 mae: 0.618241 (57.95804053562945 steps/sec)\n",
      "Step #28600\tEpoch   3 Batch  474/3125   Loss: 0.662380 mae: 0.610591 (58.49904070758346 steps/sec)\n",
      "Step #28650\tEpoch   3 Batch  524/3125   Loss: 0.686702 mae: 0.609277 (59.825471765393885 steps/sec)\n",
      "Step #28700\tEpoch   3 Batch  574/3125   Loss: 0.687153 mae: 0.621375 (59.61205230244457 steps/sec)\n",
      "Step #28750\tEpoch   3 Batch  624/3125   Loss: 0.412398 mae: 0.613048 (57.36117250101749 steps/sec)\n",
      "Step #28800\tEpoch   3 Batch  674/3125   Loss: 0.620614 mae: 0.613199 (58.70466638879379 steps/sec)\n",
      "Step #28850\tEpoch   3 Batch  724/3125   Loss: 0.537692 mae: 0.606220 (58.773436556333635 steps/sec)\n",
      "Step #28900\tEpoch   3 Batch  774/3125   Loss: 0.511193 mae: 0.609319 (56.14078671179377 steps/sec)\n",
      "Step #28950\tEpoch   3 Batch  824/3125   Loss: 0.622698 mae: 0.618538 (51.209114122117924 steps/sec)\n",
      "Step #29000\tEpoch   3 Batch  874/3125   Loss: 0.592080 mae: 0.611837 (57.230339651228604 steps/sec)\n",
      "Step #29050\tEpoch   3 Batch  924/3125   Loss: 0.713112 mae: 0.619414 (57.49276252302833 steps/sec)\n",
      "Step #29100\tEpoch   3 Batch  974/3125   Loss: 0.572216 mae: 0.616440 (59.47062783613499 steps/sec)\n",
      "Step #29150\tEpoch   3 Batch 1024/3125   Loss: 0.613745 mae: 0.608580 (59.11998513791267 steps/sec)\n",
      "Step #29200\tEpoch   3 Batch 1074/3125   Loss: 0.589762 mae: 0.608680 (59.54123918568706 steps/sec)\n",
      "Step #29250\tEpoch   3 Batch 1124/3125   Loss: 0.582862 mae: 0.615799 (59.54118847182163 steps/sec)\n",
      "Step #29300\tEpoch   3 Batch 1174/3125   Loss: 0.500262 mae: 0.613560 (59.82542056618645 steps/sec)\n",
      "Step #29350\tEpoch   3 Batch 1224/3125   Loss: 0.619422 mae: 0.604378 (58.159696559841215 steps/sec)\n",
      "Step #29400\tEpoch   3 Batch 1274/3125   Loss: 0.632474 mae: 0.620277 (60.62121108758769 steps/sec)\n",
      "Step #29450\tEpoch   3 Batch 1324/3125   Loss: 0.515181 mae: 0.613498 (59.89689455540418 steps/sec)\n",
      "Step #29500\tEpoch   3 Batch 1374/3125   Loss: 0.555639 mae: 0.606277 (59.189749089780136 steps/sec)\n",
      "Step #29550\tEpoch   3 Batch 1424/3125   Loss: 0.617369 mae: 0.603966 (60.04037329407659 steps/sec)\n",
      "Step #29600\tEpoch   3 Batch 1474/3125   Loss: 0.522555 mae: 0.612289 (58.980783557900615 steps/sec)\n",
      "Step #29650\tEpoch   3 Batch 1524/3125   Loss: 0.551052 mae: 0.605753 (59.120018470495275 steps/sec)\n",
      "Step #29700\tEpoch   3 Batch 1574/3125   Loss: 0.619991 mae: 0.613545 (59.54118847182163 steps/sec)\n",
      "Step #29750\tEpoch   3 Batch 1624/3125   Loss: 0.675606 mae: 0.604263 (60.184506525685755 steps/sec)\n",
      "Step #29800\tEpoch   3 Batch 1674/3125   Loss: 0.481736 mae: 0.616606 (59.54117156721901 steps/sec)\n",
      "Step #29850\tEpoch   3 Batch 1724/3125   Loss: 0.692086 mae: 0.610687 (59.18973238414598 steps/sec)\n",
      "Step #29900\tEpoch   3 Batch 1774/3125   Loss: 0.534521 mae: 0.609184 (59.05026905767726 steps/sec)\n",
      "Step #29950\tEpoch   3 Batch 1824/3125   Loss: 0.556107 mae: 0.609280 (59.61205230244457 steps/sec)\n",
      "Step #30000\tEpoch   3 Batch 1874/3125   Loss: 0.636937 mae: 0.613264 (60.18454106945659 steps/sec)\n",
      "Step #30050\tEpoch   3 Batch 1924/3125   Loss: 0.782638 mae: 0.615506 (59.54117156721901 steps/sec)\n",
      "Step #30100\tEpoch   3 Batch 1974/3125   Loss: 0.580784 mae: 0.611267 (59.89692876980568 steps/sec)\n",
      "Step #30150\tEpoch   3 Batch 2024/3125   Loss: 0.722239 mae: 0.616906 (59.32987281638325 steps/sec)\n",
      "Step #30200\tEpoch   3 Batch 2074/3125   Loss: 0.673306 mae: 0.605832 (58.635925811497906 steps/sec)\n",
      "Step #30250\tEpoch   3 Batch 2124/3125   Loss: 0.541013 mae: 0.612359 (60.329402856995735 steps/sec)\n",
      "Step #30300\tEpoch   3 Batch 2174/3125   Loss: 0.568743 mae: 0.609061 (59.96857382648692 steps/sec)\n",
      "Step #30350\tEpoch   3 Batch 2224/3125   Loss: 0.586041 mae: 0.617478 (59.25974486530476 steps/sec)\n",
      "Step #30400\tEpoch   3 Batch 2274/3125   Loss: 0.636522 mae: 0.613871 (59.61203535757949 steps/sec)\n",
      "Step #30450\tEpoch   3 Batch 2324/3125   Loss: 0.510924 mae: 0.617396 (58.84240105611228 steps/sec)\n",
      "Step #30500\tEpoch   3 Batch 2374/3125   Loss: 0.556342 mae: 0.607492 (60.112362127083856 steps/sec)\n",
      "Step #30550\tEpoch   3 Batch 2424/3125   Loss: 0.572713 mae: 0.612786 (58.911494493424556 steps/sec)\n",
      "Step #30600\tEpoch   3 Batch 2474/3125   Loss: 0.677595 mae: 0.608701 (59.683000087084764 steps/sec)\n",
      "Step #30650\tEpoch   3 Batch 2524/3125   Loss: 0.685174 mae: 0.610865 (58.1598094648576 steps/sec)\n",
      "Step #30700\tEpoch   3 Batch 2574/3125   Loss: 0.635403 mae: 0.616439 (57.16503770383842 steps/sec)\n",
      "Step #30750\tEpoch   3 Batch 2624/3125   Loss: 0.725948 mae: 0.615198 (57.691267627641274 steps/sec)\n",
      "Step #30800\tEpoch   3 Batch 2674/3125   Loss: 0.728294 mae: 0.617787 (52.49606057761507 steps/sec)\n",
      "Step #30850\tEpoch   3 Batch 2724/3125   Loss: 0.607829 mae: 0.606164 (58.911577238238905 steps/sec)\n",
      "Step #30900\tEpoch   3 Batch 2774/3125   Loss: 0.559719 mae: 0.617410 (58.6359094170218 steps/sec)\n",
      "Step #30950\tEpoch   3 Batch 2824/3125   Loss: 0.616064 mae: 0.620918 (61.21337898463793 steps/sec)\n",
      "Step #31000\tEpoch   3 Batch 2874/3125   Loss: 0.641140 mae: 0.606293 (59.4001164690394 steps/sec)\n",
      "Step #31050\tEpoch   3 Batch 2924/3125   Loss: 0.626364 mae: 0.612091 (59.61206924731928 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #31100\tEpoch   3 Batch 2974/3125   Loss: 0.559344 mae: 0.613591 (60.04035610483426 steps/sec)\n",
      "Step #31150\tEpoch   3 Batch 3024/3125   Loss: 0.567439 mae: 0.605221 (60.54798159151408 steps/sec)\n",
      "Step #31200\tEpoch   3 Batch 3074/3125   Loss: 0.627377 mae: 0.605516 (59.4001164690394 steps/sec)\n",
      "Step #31250\tEpoch   3 Batch 3124/3125   Loss: 0.715656 mae: 0.601663 (58.42897354392986 steps/sec)\n",
      "\n",
      "Train time for epoch #4 (31250 total steps): 53.22566604614258\n",
      "Model test set loss: 0.806341 mae: 0.704461\n",
      "Step #31300\tEpoch   4 Batch   49/3125   Loss: 0.653065 mae: 0.698928 (57.560474602080646 steps/sec)\n",
      "Step #31350\tEpoch   4 Batch   99/3125   Loss: 0.678938 mae: 0.603214 (60.256824301881764 steps/sec)\n",
      "Step #31400\tEpoch   4 Batch  149/3125   Loss: 0.571468 mae: 0.610890 (60.54796411039199 steps/sec)\n",
      "Step #31450\tEpoch   4 Batch  199/3125   Loss: 0.637329 mae: 0.604941 (59.82548883181584 steps/sec)\n",
      "Step #31500\tEpoch   4 Batch  249/3125   Loss: 0.684213 mae: 0.605060 (60.69456523469324 steps/sec)\n",
      "Step #31550\tEpoch   4 Batch  299/3125   Loss: 0.608444 mae: 0.611937 (59.050302311692604 steps/sec)\n",
      "Step #31600\tEpoch   4 Batch  349/3125   Loss: 0.612196 mae: 0.609014 (59.189749089780136 steps/sec)\n",
      "Step #31650\tEpoch   4 Batch  399/3125   Loss: 0.657414 mae: 0.606708 (51.209089113228835 steps/sec)\n",
      "Step #31700\tEpoch   4 Batch  449/3125   Loss: 0.590306 mae: 0.614701 (57.42693930574028 steps/sec)\n",
      "Step #31750\tEpoch   4 Batch  499/3125   Loss: 0.554270 mae: 0.600222 (60.112362127083856 steps/sec)\n",
      "Step #31800\tEpoch   4 Batch  549/3125   Loss: 0.581390 mae: 0.605241 (59.47062783613499 steps/sec)\n",
      "Step #31850\tEpoch   4 Batch  599/3125   Loss: 0.539861 mae: 0.617820 (60.98994305982136 steps/sec)\n",
      "Step #31900\tEpoch   4 Batch  649/3125   Loss: 0.596887 mae: 0.607407 (59.11988514039039 steps/sec)\n",
      "Step #31950\tEpoch   4 Batch  699/3125   Loss: 0.564959 mae: 0.602105 (59.47054351343066 steps/sec)\n",
      "Step #32000\tEpoch   4 Batch  749/3125   Loss: 0.572204 mae: 0.596802 (59.050302311692604 steps/sec)\n",
      "Step #32050\tEpoch   4 Batch  799/3125   Loss: 0.605640 mae: 0.612379 (60.18452379756622 steps/sec)\n",
      "Step #32100\tEpoch   4 Batch  849/3125   Loss: 0.665371 mae: 0.612543 (59.82545469898167 steps/sec)\n",
      "Step #32150\tEpoch   4 Batch  899/3125   Loss: 0.634276 mae: 0.608615 (59.05035219278585 steps/sec)\n",
      "Step #32200\tEpoch   4 Batch  949/3125   Loss: 0.579138 mae: 0.613000 (60.040407672590774 steps/sec)\n",
      "Step #32250\tEpoch   4 Batch  999/3125   Loss: 0.708757 mae: 0.605370 (59.968556678331616 steps/sec)\n",
      "Step #32300\tEpoch   4 Batch 1049/3125   Loss: 0.653670 mae: 0.608398 (60.402069477794704 steps/sec)\n",
      "Step #32350\tEpoch   4 Batch 1099/3125   Loss: 0.549063 mae: 0.603264 (57.95804053562945 steps/sec)\n",
      "Step #32400\tEpoch   4 Batch 1149/3125   Loss: 0.577992 mae: 0.610539 (59.47061097157499 steps/sec)\n",
      "Step #32450\tEpoch   4 Batch 1199/3125   Loss: 0.619066 mae: 0.600487 (59.119951805367656 steps/sec)\n",
      "Step #32500\tEpoch   4 Batch 1249/3125   Loss: 0.685348 mae: 0.608856 (59.54125609032807 steps/sec)\n",
      "Step #32550\tEpoch   4 Batch 1299/3125   Loss: 0.546372 mae: 0.616213 (60.91583824807477 steps/sec)\n",
      "Step #32600\tEpoch   4 Batch 1349/3125   Loss: 0.576276 mae: 0.596544 (59.61203535757949 steps/sec)\n",
      "Step #32650\tEpoch   4 Batch 1399/3125   Loss: 0.599485 mae: 0.603595 (59.259711374977144 steps/sec)\n",
      "Step #32700\tEpoch   4 Batch 1449/3125   Loss: 0.625905 mae: 0.601267 (60.32942021214165 steps/sec)\n",
      "Step #32750\tEpoch   4 Batch 1499/3125   Loss: 0.699394 mae: 0.607919 (59.754133976895034 steps/sec)\n",
      "Step #32800\tEpoch   4 Batch 1549/3125   Loss: 0.674751 mae: 0.606515 (61.13873054642169 steps/sec)\n",
      "Step #32850\tEpoch   4 Batch 1599/3125   Loss: 0.576850 mae: 0.600443 (60.184575613267086 steps/sec)\n",
      "Step #32900\tEpoch   4 Batch 1649/3125   Loss: 0.653819 mae: 0.608027 (60.474963680163974 steps/sec)\n",
      "Step #32950\tEpoch   4 Batch 1699/3125   Loss: 0.630050 mae: 0.609723 (59.541154662626 steps/sec)\n",
      "Step #33000\tEpoch   4 Batch 1749/3125   Loss: 0.650299 mae: 0.603944 (60.40210427176385 steps/sec)\n",
      "Step #33050\tEpoch   4 Batch 1799/3125   Loss: 0.725900 mae: 0.605227 (60.91578516556884 steps/sec)\n",
      "Step #33100\tEpoch   4 Batch 1849/3125   Loss: 0.559716 mae: 0.602280 (59.61205230244457 steps/sec)\n",
      "Step #33150\tEpoch   4 Batch 1899/3125   Loss: 0.682180 mae: 0.604108 (59.05033556574541 steps/sec)\n",
      "Step #33200\tEpoch   4 Batch 1949/3125   Loss: 0.588578 mae: 0.609071 (59.68294913144409 steps/sec)\n",
      "Step #33250\tEpoch   4 Batch 1999/3125   Loss: 0.650103 mae: 0.609933 (57.891148237525876 steps/sec)\n",
      "Step #33300\tEpoch   4 Batch 2049/3125   Loss: 0.582061 mae: 0.611156 (60.18454106945659 steps/sec)\n",
      "Step #33350\tEpoch   4 Batch 2099/3125   Loss: 0.588966 mae: 0.601004 (59.754082899717865 steps/sec)\n",
      "Step #33400\tEpoch   4 Batch 2149/3125   Loss: 0.591692 mae: 0.602051 (59.89689455540418 steps/sec)\n",
      "Step #33450\tEpoch   4 Batch 2199/3125   Loss: 0.587245 mae: 0.610964 (60.11237935758082 steps/sec)\n",
      "Step #33500\tEpoch   4 Batch 2249/3125   Loss: 0.498837 mae: 0.609380 (53.10779645462145 steps/sec)\n",
      "Step #33550\tEpoch   4 Batch 2299/3125   Loss: 0.646802 mae: 0.617097 (53.561642121275504 steps/sec)\n",
      "Step #33600\tEpoch   4 Batch 2349/3125   Loss: 0.591538 mae: 0.602856 (60.04037329407659 steps/sec)\n",
      "Step #33650\tEpoch   4 Batch 2399/3125   Loss: 0.533336 mae: 0.604203 (57.95804053562945 steps/sec)\n",
      "Step #33700\tEpoch   4 Batch 2449/3125   Loss: 0.516552 mae: 0.603702 (58.295077355602274 steps/sec)\n",
      "Step #33750\tEpoch   4 Batch 2499/3125   Loss: 0.649571 mae: 0.610912 (60.04037329407659 steps/sec)\n",
      "Step #33800\tEpoch   4 Batch 2549/3125   Loss: 0.491675 mae: 0.607617 (59.25977835567023 steps/sec)\n",
      "Step #33850\tEpoch   4 Batch 2599/3125   Loss: 0.669404 mae: 0.610085 (58.911494493424556 steps/sec)\n",
      "Step #33900\tEpoch   4 Batch 2649/3125   Loss: 0.739153 mae: 0.605763 (60.474963680163974 steps/sec)\n",
      "Step #33950\tEpoch   4 Batch 2699/3125   Loss: 0.562514 mae: 0.609465 (58.8423680358427 steps/sec)\n",
      "Step #34000\tEpoch   4 Batch 2749/3125   Loss: 0.646174 mae: 0.604322 (58.29499633354663 steps/sec)\n",
      "Step #34050\tEpoch   4 Batch 2799/3125   Loss: 0.613076 mae: 0.617727 (59.25977835567023 steps/sec)\n",
      "Step #34100\tEpoch   4 Batch 2849/3125   Loss: 0.576713 mae: 0.606921 (60.256893555594374 steps/sec)\n",
      "Step #34150\tEpoch   4 Batch 2899/3125   Loss: 0.634339 mae: 0.602139 (59.05033556574541 steps/sec)\n",
      "Step #34200\tEpoch   4 Batch 2949/3125   Loss: 0.703062 mae: 0.612191 (59.4000828198634 steps/sec)\n",
      "Step #34250\tEpoch   4 Batch 2999/3125   Loss: 0.619786 mae: 0.606703 (57.361250948087864 steps/sec)\n",
      "Step #34300\tEpoch   4 Batch 3049/3125   Loss: 0.612839 mae: 0.596809 (59.32983924674389 steps/sec)\n",
      "Step #34350\tEpoch   4 Batch 3099/3125   Loss: 0.576933 mae: 0.600176 (60.47492880220451 steps/sec)\n",
      "\n",
      "Train time for epoch #5 (34375 total steps): 52.8795382976532\n",
      "Model test set loss: 0.812929 mae: 0.703663\n",
      "WARNING:tensorflow:From E:\\Anaconda3\\envs\\dl\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: /export/assets\n"
     ]
    }
   ],
   "source": [
    "mv_net = mv_network()\n",
    "mv_net.training(features, targets_values, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将用户特征和电影特征做矩阵乘法得到一个预测评分的训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "movie_titles (InputLayer)       [(None, 15)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "movie_title_embed_layer (Embedd (None, 15, 32)       166880      movie_titles[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 15, 32, 1)    0           movie_title_embed_layer[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 14, 1, 8)     520         reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 13, 1, 8)     776         reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 12, 1, 8)     1032        reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 11, 1, 8)     1288        reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "movie_categories (InputLayer)   [(None, 18)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 1, 1, 8)      0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 1, 1, 8)      0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 1, 1, 8)      0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 1, 1, 8)      0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "uid (InputLayer)                [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "user_gender (InputLayer)        [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "user_age (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "user_job (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "movie_id (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "movie_categories_embed_layer (E (None, 18, 32)       608         movie_categories[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "pool_layer (Concatenate)        (None, 1, 1, 32)     0           max_pooling2d_4[0][0]            \n",
      "                                                                 max_pooling2d_5[0][0]            \n",
      "                                                                 max_pooling2d_6[0][0]            \n",
      "                                                                 max_pooling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "uid_embed_layer (Embedding)     (None, 1, 32)        193312      uid[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "gender_embed_layer (Embedding)  (None, 1, 16)        32          user_gender[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "age_embed_layer (Embedding)     (None, 1, 16)        112         user_age[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "job_embed_layer (Embedding)     (None, 1, 16)        336         user_job[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "movie_id_embed_layer (Embedding (None, 1, 32)        126496      movie_id[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 1, 32)        0           movie_categories_embed_layer[0][0\n",
      "__________________________________________________________________________________________________\n",
      "pool_layer_flat (Reshape)       (None, 1, 32)        0           pool_layer[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "uid_fc_layer (Dense)            (None, 1, 32)        1056        uid_embed_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "gender_fc_layer (Dense)         (None, 1, 32)        544         gender_embed_layer[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "age_fc_layer (Dense)            (None, 1, 32)        544         age_embed_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "job_fc_layer (Dense)            (None, 1, 32)        544         job_embed_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "movie_id_fc_layer (Dense)       (None, 1, 32)        1056        movie_id_embed_layer[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "movie_categories_fc_layer (Dens (None, 1, 32)        1056        lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_layer (Dropout)         (None, 1, 32)        0           pool_layer_flat[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 1, 128)       0           uid_fc_layer[0][0]               \n",
      "                                                                 gender_fc_layer[0][0]            \n",
      "                                                                 age_fc_layer[0][0]               \n",
      "                                                                 job_fc_layer[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 1, 96)        0           movie_id_fc_layer[0][0]          \n",
      "                                                                 movie_categories_fc_layer[0][0]  \n",
      "                                                                 dropout_layer[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1, 200)       25800       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1, 200)       19400       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "user_combine_layer_flat (Reshap (None, 200)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "movie_combine_layer_flat (Resha (None, 200)          0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "inference (Lambda)              (None,)              0           user_combine_layer_flat[0][0]    \n",
      "                                                                 movie_combine_layer_flat[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 1)            0           inference[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 541,392\n",
      "Trainable params: 541,392\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #21900\tEpoch   0 Batch   24/3125   Loss: 0.609366 mae: 0.633084 (19.4568707139653 steps/sec)\n",
      "Step #21950\tEpoch   0 Batch   74/3125   Loss: 0.663428 mae: 0.628300 (42.021797252410465 steps/sec)\n",
      "Step #22000\tEpoch   0 Batch  124/3125   Loss: 0.612210 mae: 0.631136 (40.87523976161152 steps/sec)\n",
      "Step #22050\tEpoch   0 Batch  174/3125   Loss: 0.641168 mae: 0.631124 (39.82825523484243 steps/sec)\n",
      "Step #22100\tEpoch   0 Batch  224/3125   Loss: 0.598328 mae: 0.624879 (43.3538703186847 steps/sec)\n",
      "Step #22150\tEpoch   0 Batch  274/3125   Loss: 0.583821 mae: 0.629070 (43.62936740546199 steps/sec)\n",
      "Step #22200\tEpoch   0 Batch  324/3125   Loss: 0.704832 mae: 0.625740 (45.24837984998563 steps/sec)\n",
      "Step #22250\tEpoch   0 Batch  374/3125   Loss: 0.582943 mae: 0.630155 (45.12264314750334 steps/sec)\n",
      "Step #22300\tEpoch   0 Batch  424/3125   Loss: 0.714045 mae: 0.632422 (43.550343829652206 steps/sec)\n",
      "Step #22350\tEpoch   0 Batch  474/3125   Loss: 0.679895 mae: 0.624665 (43.44193444078415 steps/sec)\n",
      "Step #22400\tEpoch   0 Batch  524/3125   Loss: 0.721284 mae: 0.621990 (43.2921712495172 steps/sec)\n",
      "Step #22450\tEpoch   0 Batch  574/3125   Loss: 0.706530 mae: 0.634218 (43.24310949240146 steps/sec)\n",
      "Step #22500\tEpoch   0 Batch  624/3125   Loss: 0.424224 mae: 0.625179 (44.818275811810224 steps/sec)\n",
      "Step #22550\tEpoch   0 Batch  674/3125   Loss: 0.648720 mae: 0.624974 (44.80063985288852 steps/sec)\n",
      "Step #22600\tEpoch   0 Batch  724/3125   Loss: 0.560535 mae: 0.620857 (44.695829309685976 steps/sec)\n",
      "Step #22650\tEpoch   0 Batch  774/3125   Loss: 0.521863 mae: 0.624376 (43.01313805375658 steps/sec)\n",
      "Step #22700\tEpoch   0 Batch  824/3125   Loss: 0.660026 mae: 0.627855 (43.880265982599816 steps/sec)\n",
      "Step #22750\tEpoch   0 Batch  874/3125   Loss: 0.612352 mae: 0.621916 (43.87975183059668 steps/sec)\n",
      "Step #22800\tEpoch   0 Batch  924/3125   Loss: 0.764541 mae: 0.635315 (46.492498335081756 steps/sec)\n",
      "Step #22850\tEpoch   0 Batch  974/3125   Loss: 0.604711 mae: 0.627039 (44.92117943286034 steps/sec)\n",
      "Step #22900\tEpoch   0 Batch 1024/3125   Loss: 0.633106 mae: 0.617729 (43.479122618824995 steps/sec)\n",
      "Step #22950\tEpoch   0 Batch 1074/3125   Loss: 0.643122 mae: 0.622020 (45.122555769760396 steps/sec)\n",
      "Step #23000\tEpoch   0 Batch 1124/3125   Loss: 0.618937 mae: 0.632023 (44.459680197584255 steps/sec)\n",
      "Step #23050\tEpoch   0 Batch 1174/3125   Loss: 0.530973 mae: 0.626415 (44.767597866253844 steps/sec)\n",
      "Step #23100\tEpoch   0 Batch 1224/3125   Loss: 0.644419 mae: 0.618129 (39.0933898931618 steps/sec)\n",
      "Step #23150\tEpoch   0 Batch 1274/3125   Loss: 0.666499 mae: 0.633042 (39.44969315366997 steps/sec)\n",
      "Step #23200\tEpoch   0 Batch 1324/3125   Loss: 0.538705 mae: 0.629203 (43.42367439532723 steps/sec)\n",
      "Step #23250\tEpoch   0 Batch 1374/3125   Loss: 0.597549 mae: 0.616763 (43.123050530095426 steps/sec)\n",
      "Step #23300\tEpoch   0 Batch 1424/3125   Loss: 0.644175 mae: 0.615885 (43.36354293961594 steps/sec)\n",
      "Step #23350\tEpoch   0 Batch 1474/3125   Loss: 0.524519 mae: 0.622741 (43.61129411138395 steps/sec)\n",
      "Step #23400\tEpoch   0 Batch 1524/3125   Loss: 0.583259 mae: 0.618419 (42.81115606036079 steps/sec)\n",
      "Step #23450\tEpoch   0 Batch 1574/3125   Loss: 0.650649 mae: 0.628359 (39.427561571724006 steps/sec)\n",
      "Step #23500\tEpoch   0 Batch 1624/3125   Loss: 0.708876 mae: 0.618989 (40.57719537287101 steps/sec)\n",
      "Step #23550\tEpoch   0 Batch 1674/3125   Loss: 0.506001 mae: 0.627851 (41.839687603681384 steps/sec)\n",
      "Step #23600\tEpoch   0 Batch 1724/3125   Loss: 0.723174 mae: 0.622002 (41.47305079912401 steps/sec)\n",
      "Step #23650\tEpoch   0 Batch 1774/3125   Loss: 0.565124 mae: 0.619862 (45.78995484437675 steps/sec)\n",
      "Step #23700\tEpoch   0 Batch 1824/3125   Loss: 0.561384 mae: 0.620746 (42.485623902863466 steps/sec)\n",
      "Step #23750\tEpoch   0 Batch 1874/3125   Loss: 0.658807 mae: 0.628876 (42.2707567227199 steps/sec)\n",
      "Step #23800\tEpoch   0 Batch 1924/3125   Loss: 0.811113 mae: 0.629040 (38.52003420081314 steps/sec)\n",
      "Step #23850\tEpoch   0 Batch 1974/3125   Loss: 0.584906 mae: 0.622207 (38.44890360731646 steps/sec)\n",
      "Step #23900\tEpoch   0 Batch 2024/3125   Loss: 0.735213 mae: 0.624850 (40.95734417221383 steps/sec)\n",
      "Step #23950\tEpoch   0 Batch 2074/3125   Loss: 0.700479 mae: 0.618448 (39.807026788168315 steps/sec)\n",
      "Step #24000\tEpoch   0 Batch 2124/3125   Loss: 0.550255 mae: 0.623848 (40.169631125089545 steps/sec)\n",
      "Step #24050\tEpoch   0 Batch 2174/3125   Loss: 0.604807 mae: 0.620607 (39.36538661159541 steps/sec)\n",
      "Step #24100\tEpoch   0 Batch 2224/3125   Loss: 0.615631 mae: 0.629032 (38.52782565594518 steps/sec)\n",
      "Step #24150\tEpoch   0 Batch 2274/3125   Loss: 0.675917 mae: 0.626452 (39.02519631674998 steps/sec)\n",
      "Step #24200\tEpoch   0 Batch 2324/3125   Loss: 0.524771 mae: 0.631073 (38.46050299097225 steps/sec)\n",
      "Step #24250\tEpoch   0 Batch 2374/3125   Loss: 0.586637 mae: 0.619301 (38.100975710344564 steps/sec)\n",
      "Step #24300\tEpoch   0 Batch 2424/3125   Loss: 0.592245 mae: 0.623183 (39.13482659415274 steps/sec)\n",
      "Step #24350\tEpoch   0 Batch 2474/3125   Loss: 0.699747 mae: 0.622036 (38.415667327214386 steps/sec)\n",
      "Step #24400\tEpoch   0 Batch 2524/3125   Loss: 0.712428 mae: 0.625818 (31.749159548477014 steps/sec)\n",
      "Step #24450\tEpoch   0 Batch 2574/3125   Loss: 0.665272 mae: 0.630659 (35.23336083587568 steps/sec)\n",
      "Step #24500\tEpoch   0 Batch 2624/3125   Loss: 0.751485 mae: 0.628155 (40.0395364151041 steps/sec)\n",
      "Step #24550\tEpoch   0 Batch 2674/3125   Loss: 0.742543 mae: 0.627069 (42.29573568069417 steps/sec)\n",
      "Step #24600\tEpoch   0 Batch 2724/3125   Loss: 0.625829 mae: 0.620515 (42.64745656965226 steps/sec)\n",
      "Step #24650\tEpoch   0 Batch 2774/3125   Loss: 0.581689 mae: 0.629271 (41.74053994451319 steps/sec)\n",
      "Step #24700\tEpoch   0 Batch 2824/3125   Loss: 0.630425 mae: 0.630648 (40.7922260359638 steps/sec)\n",
      "Step #24750\tEpoch   0 Batch 2874/3125   Loss: 0.681385 mae: 0.618629 (42.62993616136812 steps/sec)\n",
      "Step #24800\tEpoch   0 Batch 2924/3125   Loss: 0.659538 mae: 0.624073 (40.844852616666216 steps/sec)\n",
      "Step #24850\tEpoch   0 Batch 2974/3125   Loss: 0.591818 mae: 0.623566 (41.32458453279904 steps/sec)\n",
      "Step #24900\tEpoch   0 Batch 3024/3125   Loss: 0.582038 mae: 0.617174 (42.542169886951925 steps/sec)\n",
      "Step #24950\tEpoch   0 Batch 3074/3125   Loss: 0.617081 mae: 0.616709 (41.26974881001391 steps/sec)\n",
      "Step #25000\tEpoch   0 Batch 3124/3125   Loss: 0.761625 mae: 0.615721 (38.295500778271794 steps/sec)\n",
      "\n",
      "Train time for epoch #1 (25000 total steps): 77.30883955955505\n",
      "Model test set loss: 0.792989 mae: 0.699214\n",
      "best loss = 0.7929893136024475\n",
      "Step #25050\tEpoch   1 Batch   49/3125   Loss: 0.678055 mae: 0.694848 (38.93687929699612 steps/sec)\n",
      "Step #25100\tEpoch   1 Batch   99/3125   Loss: 0.727040 mae: 0.615834 (43.44216841289317 steps/sec)\n",
      "Step #25150\tEpoch   1 Batch  149/3125   Loss: 0.579439 mae: 0.623414 (44.31279055050162 steps/sec)\n",
      "Step #25200\tEpoch   1 Batch  199/3125   Loss: 0.642716 mae: 0.617866 (41.84655022449237 steps/sec)\n",
      "Step #25250\tEpoch   1 Batch  249/3125   Loss: 0.708335 mae: 0.618441 (38.86230823897198 steps/sec)\n",
      "Step #25300\tEpoch   1 Batch  299/3125   Loss: 0.622556 mae: 0.622305 (37.61220519961361 steps/sec)\n",
      "Step #25350\tEpoch   1 Batch  349/3125   Loss: 0.640674 mae: 0.620636 (33.119119591153016 steps/sec)\n",
      "Step #25400\tEpoch   1 Batch  399/3125   Loss: 0.686158 mae: 0.619125 (39.94601107892889 steps/sec)\n",
      "Step #25450\tEpoch   1 Batch  449/3125   Loss: 0.626322 mae: 0.629353 (36.00830895465342 steps/sec)\n",
      "Step #25500\tEpoch   1 Batch  499/3125   Loss: 0.591557 mae: 0.611336 (33.64922590728794 steps/sec)\n",
      "Step #25550\tEpoch   1 Batch  549/3125   Loss: 0.604866 mae: 0.619283 (39.53863995051335 steps/sec)\n",
      "Step #25600\tEpoch   1 Batch  599/3125   Loss: 0.536004 mae: 0.629591 (37.949953484365736 steps/sec)\n",
      "Step #25650\tEpoch   1 Batch  649/3125   Loss: 0.611122 mae: 0.617409 (42.14631166114169 steps/sec)\n",
      "Step #25700\tEpoch   1 Batch  699/3125   Loss: 0.592323 mae: 0.615194 (42.559281370532766 steps/sec)\n",
      "Step #25750\tEpoch   1 Batch  749/3125   Loss: 0.601337 mae: 0.609302 (42.18267980396633 steps/sec)\n",
      "Step #25800\tEpoch   1 Batch  799/3125   Loss: 0.636950 mae: 0.624601 (41.81489407561587 steps/sec)\n",
      "Step #25850\tEpoch   1 Batch  849/3125   Loss: 0.684817 mae: 0.620912 (42.17773378185576 steps/sec)\n",
      "Step #25900\tEpoch   1 Batch  899/3125   Loss: 0.656987 mae: 0.620243 (44.14262274895782 steps/sec)\n",
      "Step #25950\tEpoch   1 Batch  949/3125   Loss: 0.575930 mae: 0.624795 (38.9834623207625 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #26000\tEpoch   1 Batch  999/3125   Loss: 0.725587 mae: 0.614716 (36.87385799547348 steps/sec)\n",
      "Step #26050\tEpoch   1 Batch 1049/3125   Loss: 0.691710 mae: 0.619049 (43.537288427643446 steps/sec)\n",
      "Step #26100\tEpoch   1 Batch 1099/3125   Loss: 0.599234 mae: 0.612265 (42.28754819765667 steps/sec)\n",
      "Step #26150\tEpoch   1 Batch 1149/3125   Loss: 0.580234 mae: 0.625496 (43.63211781856887 steps/sec)\n",
      "Step #26200\tEpoch   1 Batch 1199/3125   Loss: 0.677211 mae: 0.612743 (42.1990192507752 steps/sec)\n",
      "Step #26250\tEpoch   1 Batch 1249/3125   Loss: 0.697173 mae: 0.621404 (43.22362637872241 steps/sec)\n",
      "Step #26300\tEpoch   1 Batch 1299/3125   Loss: 0.566858 mae: 0.626397 (43.25533773844401 steps/sec)\n",
      "Step #26350\tEpoch   1 Batch 1349/3125   Loss: 0.599054 mae: 0.610217 (42.1797527725596 steps/sec)\n",
      "Step #26400\tEpoch   1 Batch 1399/3125   Loss: 0.618869 mae: 0.614029 (43.12486839334597 steps/sec)\n",
      "Step #26450\tEpoch   1 Batch 1449/3125   Loss: 0.628100 mae: 0.612924 (41.39741380654695 steps/sec)\n",
      "Step #26500\tEpoch   1 Batch 1499/3125   Loss: 0.716145 mae: 0.615322 (43.24654269260639 steps/sec)\n",
      "Step #26550\tEpoch   1 Batch 1549/3125   Loss: 0.699556 mae: 0.617958 (39.4525949441505 steps/sec)\n",
      "Step #26600\tEpoch   1 Batch 1599/3125   Loss: 0.581797 mae: 0.612267 (38.298130339343956 steps/sec)\n",
      "Step #26650\tEpoch   1 Batch 1649/3125   Loss: 0.682265 mae: 0.620743 (40.790202813950444 steps/sec)\n",
      "Step #26700\tEpoch   1 Batch 1699/3125   Loss: 0.653963 mae: 0.616971 (42.30072648299491 steps/sec)\n",
      "Step #26750\tEpoch   1 Batch 1749/3125   Loss: 0.671497 mae: 0.615379 (41.13940638001033 steps/sec)\n",
      "Step #26800\tEpoch   1 Batch 1799/3125   Loss: 0.749692 mae: 0.613373 (41.604157495535866 steps/sec)\n",
      "Step #26850\tEpoch   1 Batch 1849/3125   Loss: 0.577735 mae: 0.615599 (36.93466929575957 steps/sec)\n",
      "Step #26900\tEpoch   1 Batch 1899/3125   Loss: 0.704752 mae: 0.620266 (42.05415548963471 steps/sec)\n",
      "Step #26950\tEpoch   1 Batch 1949/3125   Loss: 0.603198 mae: 0.620329 (45.69655503017889 steps/sec)\n",
      "Step #27000\tEpoch   1 Batch 1999/3125   Loss: 0.665643 mae: 0.620308 (43.78492283492777 steps/sec)\n",
      "Step #27050\tEpoch   1 Batch 2049/3125   Loss: 0.599672 mae: 0.620316 (41.81233464190939 steps/sec)\n",
      "Step #27100\tEpoch   1 Batch 2099/3125   Loss: 0.630014 mae: 0.611863 (44.13672342780896 steps/sec)\n",
      "Step #27150\tEpoch   1 Batch 2149/3125   Loss: 0.596577 mae: 0.611339 (43.66443162066791 steps/sec)\n",
      "Step #27200\tEpoch   1 Batch 2199/3125   Loss: 0.598569 mae: 0.620259 (44.34400162392584 steps/sec)\n",
      "Step #27250\tEpoch   1 Batch 2249/3125   Loss: 0.512979 mae: 0.618846 (41.57291632595007 steps/sec)\n",
      "Step #27300\tEpoch   1 Batch 2299/3125   Loss: 0.658790 mae: 0.629632 (36.64178048324507 steps/sec)\n",
      "Step #27350\tEpoch   1 Batch 2349/3125   Loss: 0.602995 mae: 0.615837 (39.195981777899554 steps/sec)\n",
      "Step #27400\tEpoch   1 Batch 2399/3125   Loss: 0.546008 mae: 0.614431 (42.84823466626566 steps/sec)\n",
      "Step #27450\tEpoch   1 Batch 2449/3125   Loss: 0.536232 mae: 0.613800 (43.33796783840842 steps/sec)\n",
      "Step #27500\tEpoch   1 Batch 2499/3125   Loss: 0.712905 mae: 0.622575 (41.785508632455645 steps/sec)\n",
      "Step #27550\tEpoch   1 Batch 2549/3125   Loss: 0.477900 mae: 0.620527 (35.35081922999124 steps/sec)\n",
      "Step #27600\tEpoch   1 Batch 2599/3125   Loss: 0.674825 mae: 0.621105 (38.451589528871075 steps/sec)\n",
      "Step #27650\tEpoch   1 Batch 2649/3125   Loss: 0.730208 mae: 0.617330 (41.50082382168565 steps/sec)\n",
      "Step #27700\tEpoch   1 Batch 2699/3125   Loss: 0.600276 mae: 0.620648 (42.3574226659409 steps/sec)\n",
      "Step #27750\tEpoch   1 Batch 2749/3125   Loss: 0.693373 mae: 0.616734 (37.608003091630806 steps/sec)\n",
      "Step #27800\tEpoch   1 Batch 2799/3125   Loss: 0.612445 mae: 0.628202 (43.05665738664827 steps/sec)\n",
      "Step #27850\tEpoch   1 Batch 2849/3125   Loss: 0.586748 mae: 0.615332 (42.085060244300415 steps/sec)\n",
      "Step #27900\tEpoch   1 Batch 2899/3125   Loss: 0.671222 mae: 0.612474 (42.491563594440635 steps/sec)\n",
      "Step #27950\tEpoch   1 Batch 2949/3125   Loss: 0.736268 mae: 0.621914 (39.819535829383476 steps/sec)\n",
      "Step #28000\tEpoch   1 Batch 2999/3125   Loss: 0.617566 mae: 0.615185 (40.62318460746649 steps/sec)\n",
      "Step #28050\tEpoch   1 Batch 3049/3125   Loss: 0.633713 mae: 0.606756 (41.44383246048053 steps/sec)\n",
      "Step #28100\tEpoch   1 Batch 3099/3125   Loss: 0.603504 mae: 0.611278 (43.1422389301994 steps/sec)\n",
      "\n",
      "Train time for epoch #2 (28125 total steps): 76.6219527721405\n",
      "Model test set loss: 0.799114 mae: 0.698628\n",
      "Step #28150\tEpoch   2 Batch   24/3125   Loss: 0.581509 mae: 0.696112 (79.13458621309019 steps/sec)\n",
      "Step #28200\tEpoch   2 Batch   74/3125   Loss: 0.644484 mae: 0.614254 (43.59260147692753 steps/sec)\n",
      "Step #28250\tEpoch   2 Batch  124/3125   Loss: 0.599568 mae: 0.616398 (43.313970479651445 steps/sec)\n",
      "Step #28300\tEpoch   2 Batch  174/3125   Loss: 0.598087 mae: 0.615289 (43.3125481215654 steps/sec)\n",
      "Step #28350\tEpoch   2 Batch  224/3125   Loss: 0.566600 mae: 0.609309 (42.553098237573145 steps/sec)\n",
      "Step #28400\tEpoch   2 Batch  274/3125   Loss: 0.565346 mae: 0.615418 (43.16038735749564 steps/sec)\n",
      "Step #28450\tEpoch   2 Batch  324/3125   Loss: 0.693265 mae: 0.612619 (42.07541767910199 steps/sec)\n",
      "Step #28500\tEpoch   2 Batch  374/3125   Loss: 0.541619 mae: 0.614637 (42.302228219814445 steps/sec)\n",
      "Step #28550\tEpoch   2 Batch  424/3125   Loss: 0.705717 mae: 0.617839 (40.52025237824276 steps/sec)\n",
      "Step #28600\tEpoch   2 Batch  474/3125   Loss: 0.658100 mae: 0.609930 (38.65199999926277 steps/sec)\n",
      "Step #28650\tEpoch   2 Batch  524/3125   Loss: 0.685314 mae: 0.609352 (38.4757797202021 steps/sec)\n",
      "Step #28700\tEpoch   2 Batch  574/3125   Loss: 0.687197 mae: 0.621117 (41.12369156917574 steps/sec)\n",
      "Step #28750\tEpoch   2 Batch  624/3125   Loss: 0.411033 mae: 0.612958 (46.792011975387894 steps/sec)\n",
      "Step #28800\tEpoch   2 Batch  674/3125   Loss: 0.612329 mae: 0.612924 (45.32619636167467 steps/sec)\n",
      "Step #28850\tEpoch   2 Batch  724/3125   Loss: 0.540069 mae: 0.606742 (47.326954323885175 steps/sec)\n",
      "Step #28900\tEpoch   2 Batch  774/3125   Loss: 0.510810 mae: 0.608482 (40.72328002731 steps/sec)\n",
      "Step #28950\tEpoch   2 Batch  824/3125   Loss: 0.628381 mae: 0.617803 (41.61922577577417 steps/sec)\n",
      "Step #29000\tEpoch   2 Batch  874/3125   Loss: 0.595418 mae: 0.611294 (47.36666527385186 steps/sec)\n",
      "Step #29050\tEpoch   2 Batch  924/3125   Loss: 0.724120 mae: 0.619936 (43.05146011264418 steps/sec)\n",
      "Step #29100\tEpoch   2 Batch  974/3125   Loss: 0.577812 mae: 0.617017 (45.374475645820006 steps/sec)\n",
      "Step #29150\tEpoch   2 Batch 1024/3125   Loss: 0.621249 mae: 0.608767 (47.01849888011264 steps/sec)\n",
      "Step #29200\tEpoch   2 Batch 1074/3125   Loss: 0.585675 mae: 0.608762 (47.27191825548834 steps/sec)\n",
      "Step #29250\tEpoch   2 Batch 1124/3125   Loss: 0.585107 mae: 0.616051 (46.81437536916386 steps/sec)\n",
      "Step #29300\tEpoch   2 Batch 1174/3125   Loss: 0.499781 mae: 0.613900 (46.00455186105307 steps/sec)\n",
      "Step #29350\tEpoch   2 Batch 1224/3125   Loss: 0.618429 mae: 0.604961 (45.79139459298943 steps/sec)\n",
      "Step #29400\tEpoch   2 Batch 1274/3125   Loss: 0.633417 mae: 0.620770 (40.75571991403246 steps/sec)\n",
      "Step #29450\tEpoch   2 Batch 1324/3125   Loss: 0.516792 mae: 0.613154 (45.382762984545444 steps/sec)\n",
      "Step #29500\tEpoch   2 Batch 1374/3125   Loss: 0.551812 mae: 0.606656 (43.521395638505155 steps/sec)\n",
      "Step #29550\tEpoch   2 Batch 1424/3125   Loss: 0.612020 mae: 0.603541 (41.843410838792835 steps/sec)\n",
      "Step #29600\tEpoch   2 Batch 1474/3125   Loss: 0.518835 mae: 0.612089 (39.13590015377036 steps/sec)\n",
      "Step #29650\tEpoch   2 Batch 1524/3125   Loss: 0.557091 mae: 0.605345 (44.208579384274145 steps/sec)\n",
      "Step #29700\tEpoch   2 Batch 1574/3125   Loss: 0.611916 mae: 0.613698 (43.462650735246875 steps/sec)\n",
      "Step #29750\tEpoch   2 Batch 1624/3125   Loss: 0.678918 mae: 0.603960 (40.073062592567894 steps/sec)\n",
      "Step #29800\tEpoch   2 Batch 1674/3125   Loss: 0.480617 mae: 0.616848 (42.253237920872294 steps/sec)\n",
      "Step #29850\tEpoch   2 Batch 1724/3125   Loss: 0.696373 mae: 0.611009 (46.57723242464341 steps/sec)\n",
      "Step #29900\tEpoch   2 Batch 1774/3125   Loss: 0.538860 mae: 0.609350 (46.09729098580825 steps/sec)\n",
      "Step #29950\tEpoch   2 Batch 1824/3125   Loss: 0.551426 mae: 0.609231 (46.198056561972614 steps/sec)\n",
      "Step #30000\tEpoch   2 Batch 1874/3125   Loss: 0.634033 mae: 0.612931 (45.090142572996335 steps/sec)\n",
      "Step #30050\tEpoch   2 Batch 1924/3125   Loss: 0.778478 mae: 0.615778 (39.16427953686787 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #30100\tEpoch   2 Batch 1974/3125   Loss: 0.581796 mae: 0.612082 (44.599625619979186 steps/sec)\n",
      "Step #30150\tEpoch   2 Batch 2024/3125   Loss: 0.719188 mae: 0.617075 (45.51120987675755 steps/sec)\n",
      "Step #30200\tEpoch   2 Batch 2074/3125   Loss: 0.674238 mae: 0.605665 (44.48971114756679 steps/sec)\n",
      "Step #30250\tEpoch   2 Batch 2124/3125   Loss: 0.545744 mae: 0.612997 (40.70864004950681 steps/sec)\n",
      "Step #30300\tEpoch   2 Batch 2174/3125   Loss: 0.568136 mae: 0.609232 (35.827359834546655 steps/sec)\n",
      "Step #30350\tEpoch   2 Batch 2224/3125   Loss: 0.587303 mae: 0.617807 (39.234619301173005 steps/sec)\n",
      "Step #30400\tEpoch   2 Batch 2274/3125   Loss: 0.640620 mae: 0.614160 (45.942620796018154 steps/sec)\n",
      "Step #30450\tEpoch   2 Batch 2324/3125   Loss: 0.506651 mae: 0.617400 (46.28870714659522 steps/sec)\n",
      "Step #30500\tEpoch   2 Batch 2374/3125   Loss: 0.557591 mae: 0.607143 (46.10328013044282 steps/sec)\n",
      "Step #30550\tEpoch   2 Batch 2424/3125   Loss: 0.571835 mae: 0.612330 (44.82077584119421 steps/sec)\n",
      "Step #30600\tEpoch   2 Batch 2474/3125   Loss: 0.672886 mae: 0.608828 (45.5387229148924 steps/sec)\n",
      "Step #30650\tEpoch   2 Batch 2524/3125   Loss: 0.684290 mae: 0.611240 (44.98159148650489 steps/sec)\n",
      "Step #30700\tEpoch   2 Batch 2574/3125   Loss: 0.636055 mae: 0.616767 (42.108137585846585 steps/sec)\n",
      "Step #30750\tEpoch   2 Batch 2624/3125   Loss: 0.739559 mae: 0.615553 (40.17598757374164 steps/sec)\n",
      "Step #30800\tEpoch   2 Batch 2674/3125   Loss: 0.720823 mae: 0.617565 (46.036504719332676 steps/sec)\n",
      "Step #30850\tEpoch   2 Batch 2724/3125   Loss: 0.609814 mae: 0.606551 (46.78650013932309 steps/sec)\n",
      "Step #30900\tEpoch   2 Batch 2774/3125   Loss: 0.557766 mae: 0.617924 (44.8806874443981 steps/sec)\n",
      "Step #30950\tEpoch   2 Batch 2824/3125   Loss: 0.618667 mae: 0.621540 (38.51878899239617 steps/sec)\n",
      "Step #31000\tEpoch   2 Batch 2874/3125   Loss: 0.636636 mae: 0.606634 (44.32315813322783 steps/sec)\n",
      "Step #31050\tEpoch   2 Batch 2924/3125   Loss: 0.622670 mae: 0.611873 (39.55184612825487 steps/sec)\n",
      "Step #31100\tEpoch   2 Batch 2974/3125   Loss: 0.566122 mae: 0.614110 (46.119623918098185 steps/sec)\n",
      "Step #31150\tEpoch   2 Batch 3024/3125   Loss: 0.575734 mae: 0.605269 (46.183864082613134 steps/sec)\n",
      "Step #31200\tEpoch   2 Batch 3074/3125   Loss: 0.628276 mae: 0.605622 (46.09798001290738 steps/sec)\n",
      "Step #31250\tEpoch   2 Batch 3124/3125   Loss: 0.716398 mae: 0.601610 (46.21152458772921 steps/sec)\n",
      "\n",
      "Train time for epoch #3 (31250 total steps): 72.12196803092957\n",
      "Model test set loss: 0.806312 mae: 0.704250\n",
      "Step #31300\tEpoch   3 Batch   49/3125   Loss: 0.655325 mae: 0.698756 (45.165051546236334 steps/sec)\n",
      "Step #31350\tEpoch   3 Batch   99/3125   Loss: 0.664410 mae: 0.602423 (47.44824115987043 steps/sec)\n",
      "Step #31400\tEpoch   3 Batch  149/3125   Loss: 0.570591 mae: 0.610577 (47.22621813193198 steps/sec)\n",
      "Step #31450\tEpoch   3 Batch  199/3125   Loss: 0.635310 mae: 0.605750 (46.69998447900334 steps/sec)\n",
      "Step #31500\tEpoch   3 Batch  249/3125   Loss: 0.686741 mae: 0.605191 (47.269041427624444 steps/sec)\n",
      "Step #31550\tEpoch   3 Batch  299/3125   Loss: 0.602872 mae: 0.611358 (47.09277637492744 steps/sec)\n",
      "Step #31600\tEpoch   3 Batch  349/3125   Loss: 0.616014 mae: 0.609767 (46.78735605899197 steps/sec)\n",
      "Step #31650\tEpoch   3 Batch  399/3125   Loss: 0.656373 mae: 0.606411 (48.41436553497921 steps/sec)\n",
      "Step #31700\tEpoch   3 Batch  449/3125   Loss: 0.593986 mae: 0.615055 (46.33265095020341 steps/sec)\n",
      "Step #31750\tEpoch   3 Batch  499/3125   Loss: 0.561375 mae: 0.599967 (46.39161551681536 steps/sec)\n",
      "Step #31800\tEpoch   3 Batch  549/3125   Loss: 0.581645 mae: 0.605374 (41.720353441470806 steps/sec)\n",
      "Step #31850\tEpoch   3 Batch  599/3125   Loss: 0.537646 mae: 0.618001 (45.57221031148097 steps/sec)\n",
      "Step #31900\tEpoch   3 Batch  649/3125   Loss: 0.595129 mae: 0.607889 (46.80742695434743 steps/sec)\n",
      "Step #31950\tEpoch   3 Batch  699/3125   Loss: 0.568333 mae: 0.603053 (41.845205908585726 steps/sec)\n",
      "Step #32000\tEpoch   3 Batch  749/3125   Loss: 0.566105 mae: 0.595865 (46.96020226579429 steps/sec)\n",
      "Step #32050\tEpoch   3 Batch  799/3125   Loss: 0.607264 mae: 0.611992 (45.320838338001955 steps/sec)\n",
      "Step #32100\tEpoch   3 Batch  849/3125   Loss: 0.669375 mae: 0.611804 (41.60175583582522 steps/sec)\n",
      "Step #32150\tEpoch   3 Batch  899/3125   Loss: 0.624958 mae: 0.609055 (39.78570774061447 steps/sec)\n",
      "Step #32200\tEpoch   3 Batch  949/3125   Loss: 0.574711 mae: 0.612948 (45.17014902484135 steps/sec)\n",
      "Step #32250\tEpoch   3 Batch  999/3125   Loss: 0.706966 mae: 0.605457 (46.826636776006495 steps/sec)\n",
      "Step #32300\tEpoch   3 Batch 1049/3125   Loss: 0.654778 mae: 0.608949 (40.05705759164704 steps/sec)\n",
      "Step #32350\tEpoch   3 Batch 1099/3125   Loss: 0.550867 mae: 0.602939 (42.37864158108364 steps/sec)\n",
      "Step #32400\tEpoch   3 Batch 1149/3125   Loss: 0.561577 mae: 0.610509 (42.08500112579754 steps/sec)\n",
      "Step #32450\tEpoch   3 Batch 1199/3125   Loss: 0.614424 mae: 0.600851 (38.92400828462617 steps/sec)\n",
      "Step #32500\tEpoch   3 Batch 1249/3125   Loss: 0.676204 mae: 0.609070 (37.437490438243955 steps/sec)\n",
      "Step #32550\tEpoch   3 Batch 1299/3125   Loss: 0.547041 mae: 0.615841 (35.277023151286585 steps/sec)\n",
      "Step #32600\tEpoch   3 Batch 1349/3125   Loss: 0.575645 mae: 0.596777 (35.541250452918966 steps/sec)\n",
      "Step #32650\tEpoch   3 Batch 1399/3125   Loss: 0.588777 mae: 0.603811 (36.15407172744461 steps/sec)\n",
      "Step #32700\tEpoch   3 Batch 1449/3125   Loss: 0.625551 mae: 0.600933 (35.76298821168859 steps/sec)\n",
      "Step #32750\tEpoch   3 Batch 1499/3125   Loss: 0.701470 mae: 0.607317 (38.02430804673478 steps/sec)\n",
      "Step #32800\tEpoch   3 Batch 1549/3125   Loss: 0.682014 mae: 0.606801 (38.87988802217309 steps/sec)\n",
      "Step #32850\tEpoch   3 Batch 1599/3125   Loss: 0.580458 mae: 0.600206 (37.957337379891925 steps/sec)\n",
      "Step #32900\tEpoch   3 Batch 1649/3125   Loss: 0.645927 mae: 0.608312 (38.02041314334053 steps/sec)\n",
      "Step #32950\tEpoch   3 Batch 1699/3125   Loss: 0.635316 mae: 0.609444 (36.37117274048977 steps/sec)\n",
      "Step #33000\tEpoch   3 Batch 1749/3125   Loss: 0.651010 mae: 0.603685 (37.56756117676648 steps/sec)\n",
      "Step #33050\tEpoch   3 Batch 1799/3125   Loss: 0.722795 mae: 0.605313 (37.46664513279775 steps/sec)\n",
      "Step #33100\tEpoch   3 Batch 1849/3125   Loss: 0.562007 mae: 0.601617 (37.39898147324187 steps/sec)\n",
      "Step #33150\tEpoch   3 Batch 1899/3125   Loss: 0.674813 mae: 0.604228 (42.901098757226634 steps/sec)\n",
      "Step #33200\tEpoch   3 Batch 1949/3125   Loss: 0.595756 mae: 0.610035 (43.51443321215743 steps/sec)\n",
      "Step #33250\tEpoch   3 Batch 1999/3125   Loss: 0.652484 mae: 0.610746 (42.00645294136028 steps/sec)\n",
      "Step #33300\tEpoch   3 Batch 2049/3125   Loss: 0.592129 mae: 0.611847 (44.33417724944232 steps/sec)\n",
      "Step #33350\tEpoch   3 Batch 2099/3125   Loss: 0.589379 mae: 0.601264 (39.71990876257682 steps/sec)\n",
      "Step #33400\tEpoch   3 Batch 2149/3125   Loss: 0.593580 mae: 0.602275 (35.36597931450961 steps/sec)\n",
      "Step #33450\tEpoch   3 Batch 2199/3125   Loss: 0.582685 mae: 0.611105 (41.54709617346686 steps/sec)\n",
      "Step #33500\tEpoch   3 Batch 2249/3125   Loss: 0.494660 mae: 0.609208 (42.685763752134385 steps/sec)\n",
      "Step #33550\tEpoch   3 Batch 2299/3125   Loss: 0.651660 mae: 0.617614 (40.2665888914127 steps/sec)\n",
      "Step #33600\tEpoch   3 Batch 2349/3125   Loss: 0.584559 mae: 0.602186 (44.44083725809101 steps/sec)\n",
      "Step #33650\tEpoch   3 Batch 2399/3125   Loss: 0.530678 mae: 0.604367 (43.535272951774424 steps/sec)\n",
      "Step #33700\tEpoch   3 Batch 2449/3125   Loss: 0.518428 mae: 0.603364 (45.29835237835833 steps/sec)\n",
      "Step #33750\tEpoch   3 Batch 2499/3125   Loss: 0.645942 mae: 0.610985 (44.130472797334605 steps/sec)\n",
      "Step #33800\tEpoch   3 Batch 2549/3125   Loss: 0.489754 mae: 0.607966 (44.93957490286298 steps/sec)\n",
      "Step #33850\tEpoch   3 Batch 2599/3125   Loss: 0.670283 mae: 0.609935 (44.2644258431477 steps/sec)\n",
      "Step #33900\tEpoch   3 Batch 2649/3125   Loss: 0.727009 mae: 0.605786 (43.06964721799898 steps/sec)\n",
      "Step #33950\tEpoch   3 Batch 2699/3125   Loss: 0.556971 mae: 0.609129 (38.6170038028639 steps/sec)\n",
      "Step #34000\tEpoch   3 Batch 2749/3125   Loss: 0.642808 mae: 0.604496 (45.83846619914966 steps/sec)\n",
      "Step #34050\tEpoch   3 Batch 2799/3125   Loss: 0.602943 mae: 0.618320 (44.27738818236232 steps/sec)\n",
      "Step #34100\tEpoch   3 Batch 2849/3125   Loss: 0.577342 mae: 0.607756 (45.96693995688583 steps/sec)\n",
      "Step #34150\tEpoch   3 Batch 2899/3125   Loss: 0.633952 mae: 0.602579 (45.73085341936013 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #34200\tEpoch   3 Batch 2949/3125   Loss: 0.711594 mae: 0.611860 (43.3087913985103 steps/sec)\n",
      "Step #34250\tEpoch   3 Batch 2999/3125   Loss: 0.617143 mae: 0.606910 (38.92084422973029 steps/sec)\n",
      "Step #34300\tEpoch   3 Batch 3049/3125   Loss: 0.607009 mae: 0.596963 (41.173984090363376 steps/sec)\n",
      "Step #34350\tEpoch   3 Batch 3099/3125   Loss: 0.572505 mae: 0.600058 (41.63199074435264 steps/sec)\n",
      "\n",
      "Train time for epoch #4 (34375 total steps): 74.5869665145874\n",
      "Model test set loss: 0.813084 mae: 0.703456\n",
      "Step #34400\tEpoch   4 Batch   24/3125   Loss: 0.554919 mae: 0.700476 (80.00091553782124 steps/sec)\n",
      "Step #34450\tEpoch   4 Batch   74/3125   Loss: 0.646027 mae: 0.602859 (44.670257494612585 steps/sec)\n",
      "Step #34500\tEpoch   4 Batch  124/3125   Loss: 0.578570 mae: 0.604482 (38.99296485514057 steps/sec)\n",
      "Step #34550\tEpoch   4 Batch  174/3125   Loss: 0.567312 mae: 0.603299 (40.675927899627546 steps/sec)\n",
      "Step #34600\tEpoch   4 Batch  224/3125   Loss: 0.543727 mae: 0.599157 (45.199004530133614 steps/sec)\n",
      "Step #34650\tEpoch   4 Batch  274/3125   Loss: 0.559509 mae: 0.603616 (46.46740382808706 steps/sec)\n",
      "Step #34700\tEpoch   4 Batch  324/3125   Loss: 0.682666 mae: 0.603712 (46.223788030471546 steps/sec)\n",
      "Step #34750\tEpoch   4 Batch  374/3125   Loss: 0.522857 mae: 0.604477 (46.56083152461573 steps/sec)\n",
      "Step #34800\tEpoch   4 Batch  424/3125   Loss: 0.672461 mae: 0.606928 (43.42422287249154 steps/sec)\n",
      "Step #34850\tEpoch   4 Batch  474/3125   Loss: 0.631616 mae: 0.598709 (45.41677864834131 steps/sec)\n",
      "Step #34900\tEpoch   4 Batch  524/3125   Loss: 0.664275 mae: 0.597583 (44.678156308526646 steps/sec)\n",
      "Step #34950\tEpoch   4 Batch  574/3125   Loss: 0.677298 mae: 0.609089 (45.470732153651745 steps/sec)\n",
      "Step #35000\tEpoch   4 Batch  624/3125   Loss: 0.411225 mae: 0.603596 (46.01664510089905 steps/sec)\n",
      "Step #35050\tEpoch   4 Batch  674/3125   Loss: 0.587065 mae: 0.602236 (43.987490587527084 steps/sec)\n",
      "Step #35100\tEpoch   4 Batch  724/3125   Loss: 0.528678 mae: 0.596115 (35.94478924031215 steps/sec)\n",
      "Step #35150\tEpoch   4 Batch  774/3125   Loss: 0.492533 mae: 0.594527 (43.453978278729714 steps/sec)\n",
      "Step #35200\tEpoch   4 Batch  824/3125   Loss: 0.605684 mae: 0.608672 (40.73892766285454 steps/sec)\n",
      "Step #35250\tEpoch   4 Batch  874/3125   Loss: 0.578162 mae: 0.602616 (45.00952063720449 steps/sec)\n",
      "Step #35300\tEpoch   4 Batch  924/3125   Loss: 0.685505 mae: 0.607910 (44.939141555321605 steps/sec)\n",
      "Step #35350\tEpoch   4 Batch  974/3125   Loss: 0.547309 mae: 0.606151 (44.324113655952296 steps/sec)\n",
      "Step #35400\tEpoch   4 Batch 1024/3125   Loss: 0.616766 mae: 0.600163 (42.870482878398114 steps/sec)\n",
      "Step #35450\tEpoch   4 Batch 1074/3125   Loss: 0.563763 mae: 0.600138 (44.81037525381625 steps/sec)\n",
      "Step #35500\tEpoch   4 Batch 1124/3125   Loss: 0.538337 mae: 0.605764 (44.767234723194385 steps/sec)\n",
      "Step #35550\tEpoch   4 Batch 1174/3125   Loss: 0.476918 mae: 0.602368 (45.822901749447084 steps/sec)\n",
      "Step #35600\tEpoch   4 Batch 1224/3125   Loss: 0.606114 mae: 0.594046 (43.30591168547112 steps/sec)\n",
      "Step #35650\tEpoch   4 Batch 1274/3125   Loss: 0.618780 mae: 0.610752 (44.15028957711846 steps/sec)\n",
      "Step #35700\tEpoch   4 Batch 1324/3125   Loss: 0.491277 mae: 0.602813 (43.60822894386737 steps/sec)\n",
      "Step #35750\tEpoch   4 Batch 1374/3125   Loss: 0.513191 mae: 0.594902 (44.794276317293075 steps/sec)\n",
      "Step #35800\tEpoch   4 Batch 1424/3125   Loss: 0.602094 mae: 0.593027 (44.4353287170838 steps/sec)\n",
      "Step #35850\tEpoch   4 Batch 1474/3125   Loss: 0.497471 mae: 0.602979 (45.050206824966104 steps/sec)\n",
      "Step #35900\tEpoch   4 Batch 1524/3125   Loss: 0.545900 mae: 0.599459 (39.51911902684109 steps/sec)\n",
      "Step #35950\tEpoch   4 Batch 1574/3125   Loss: 0.605896 mae: 0.603210 (44.110822248584434 steps/sec)\n",
      "Step #36000\tEpoch   4 Batch 1624/3125   Loss: 0.662535 mae: 0.594977 (43.83852121248101 steps/sec)\n",
      "Step #36050\tEpoch   4 Batch 1674/3125   Loss: 0.468871 mae: 0.608176 (44.52947204231713 steps/sec)\n",
      "Step #36100\tEpoch   4 Batch 1724/3125   Loss: 0.670085 mae: 0.601582 (45.5338088452512 steps/sec)\n",
      "Step #36150\tEpoch   4 Batch 1774/3125   Loss: 0.517658 mae: 0.600548 (45.96722206915538 steps/sec)\n",
      "Step #36200\tEpoch   4 Batch 1824/3125   Loss: 0.534687 mae: 0.599319 (46.46698169750171 steps/sec)\n",
      "Step #36250\tEpoch   4 Batch 1874/3125   Loss: 0.607732 mae: 0.600026 (47.12006499715997 steps/sec)\n",
      "Step #36300\tEpoch   4 Batch 1924/3125   Loss: 0.733142 mae: 0.602652 (46.69954771303482 steps/sec)\n",
      "Step #36350\tEpoch   4 Batch 1974/3125   Loss: 0.568153 mae: 0.601334 (46.982284683496324 steps/sec)\n",
      "Step #36400\tEpoch   4 Batch 2024/3125   Loss: 0.693205 mae: 0.609330 (44.56079983494363 steps/sec)\n",
      "Step #36450\tEpoch   4 Batch 2074/3125   Loss: 0.653250 mae: 0.595823 (41.91084408957039 steps/sec)\n",
      "Step #36500\tEpoch   4 Batch 2124/3125   Loss: 0.546438 mae: 0.603082 (36.718015052769786 steps/sec)\n",
      "Step #36550\tEpoch   4 Batch 2174/3125   Loss: 0.540723 mae: 0.600389 (41.985840365668764 steps/sec)\n",
      "Step #36600\tEpoch   4 Batch 2224/3125   Loss: 0.563290 mae: 0.608486 (46.67617186091553 steps/sec)\n",
      "Step #36650\tEpoch   4 Batch 2274/3125   Loss: 0.628123 mae: 0.605827 (44.503164652321146 steps/sec)\n",
      "Step #36700\tEpoch   4 Batch 2324/3125   Loss: 0.487871 mae: 0.606355 (43.95779267804707 steps/sec)\n",
      "Step #36750\tEpoch   4 Batch 2374/3125   Loss: 0.524804 mae: 0.594063 (45.51523000236784 steps/sec)\n",
      "Step #36800\tEpoch   4 Batch 2424/3125   Loss: 0.566804 mae: 0.603408 (45.656910032797875 steps/sec)\n",
      "Step #36850\tEpoch   4 Batch 2474/3125   Loss: 0.656738 mae: 0.598996 (43.89406077076319 steps/sec)\n",
      "Step #36900\tEpoch   4 Batch 2524/3125   Loss: 0.642260 mae: 0.600920 (44.760202377433274 steps/sec)\n",
      "Step #36950\tEpoch   4 Batch 2574/3125   Loss: 0.613813 mae: 0.606905 (42.55566281069013 steps/sec)\n",
      "Step #37000\tEpoch   4 Batch 2624/3125   Loss: 0.707992 mae: 0.604559 (44.70454716957804 steps/sec)\n",
      "Step #37050\tEpoch   4 Batch 2674/3125   Loss: 0.692126 mae: 0.607793 (43.495462654893394 steps/sec)\n",
      "Step #37100\tEpoch   4 Batch 2724/3125   Loss: 0.588136 mae: 0.596428 (44.26437912890554 steps/sec)\n",
      "Step #37150\tEpoch   4 Batch 2774/3125   Loss: 0.541092 mae: 0.605889 (43.43352211842471 steps/sec)\n",
      "Step #37200\tEpoch   4 Batch 2824/3125   Loss: 0.590363 mae: 0.612325 (44.871411108945324 steps/sec)\n",
      "Step #37250\tEpoch   4 Batch 2874/3125   Loss: 0.599295 mae: 0.599927 (42.47120339113143 steps/sec)\n",
      "Step #37300\tEpoch   4 Batch 2924/3125   Loss: 0.591029 mae: 0.602471 (44.68493438481274 steps/sec)\n",
      "Step #37350\tEpoch   4 Batch 2974/3125   Loss: 0.529399 mae: 0.606009 (42.73847133306243 steps/sec)\n",
      "Step #37400\tEpoch   4 Batch 3024/3125   Loss: 0.559190 mae: 0.596691 (44.971010061024906 steps/sec)\n",
      "Step #37450\tEpoch   4 Batch 3074/3125   Loss: 0.622469 mae: 0.596996 (43.40126162896701 steps/sec)\n",
      "Step #37500\tEpoch   4 Batch 3124/3125   Loss: 0.679975 mae: 0.590744 (47.08840932197605 steps/sec)\n",
      "\n",
      "Train time for epoch #5 (37500 total steps): 71.18730163574219\n",
      "Model test set loss: 0.819639 mae: 0.709124\n",
      "INFO:tensorflow:Assets written to: /export/assets\n"
     ]
    }
   ],
   "source": [
    "mv_net=mv_network()\n",
    "mv_net.training(features, targets_values, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 指定用户和电影进行评分\n",
    "\n",
    "这部分就是对网络做正向传播，计算得到预测的评分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rating_movie(mv_net, user_id_val, movie_id_val):\n",
    "    categories = np.zeros([1, 18])\n",
    "    categories[0] = movies.values[movieid2idx[movie_id_val]][2]\n",
    "\n",
    "    titles = np.zeros([1, sentences_size])\n",
    "    titles[0] = movies.values[movieid2idx[movie_id_val]][1]\n",
    "\n",
    "    inference_val = mv_net.model([\n",
    "        np.reshape(users.values[user_id_val - 1][0], [1, 1]),\n",
    "        np.reshape(users.values[user_id_val - 1][1], [1, 1]),\n",
    "        np.reshape(users.values[user_id_val - 1][2], [1, 1]),\n",
    "        np.reshape(users.values[user_id_val - 1][3], [1, 1]),\n",
    "        np.reshape(movies.values[movieid2idx[movie_id_val]][0], [1, 1]),\n",
    "        categories, titles\n",
    "    ])\n",
    "\n",
    "    return (inference_val.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.8851764]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating_movie(mv_net, 234, 1401)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成Movie特征矩阵\n",
    "\n",
    "将训练好的电影特征组合成电影特征矩阵并保存到本地\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_layer_model = keras.models.Model(\n",
    "    inputs=[\n",
    "        mv_net.model.input[4], mv_net.model.input[5], mv_net.model.input[6]\n",
    "    ],\n",
    "    outputs=mv_net.model.get_layer(\"movie_combine_layer_flat\").output)\n",
    "movie_matrics = []\n",
    "\n",
    "for item in movies.values:\n",
    "    categories = np.zeros([1, 18])\n",
    "    categories[0] = item.take(2)\n",
    "\n",
    "    titles = np.zeros([1, sentences_size])\n",
    "    titles[0] = item.take(1)\n",
    "\n",
    "    movie_combine_layer_flat_val = movie_layer_model(\n",
    "        [np.reshape(item.take(0), [1, 1]), categories, titles])\n",
    "    movie_matrics.append(movie_combine_layer_flat_val)\n",
    "\n",
    "pickle.dump((np.array(movie_matrics).reshape(-1, 200)),\n",
    "            open('movie_matrics.p', 'wb'))\n",
    "movie_matrics = pickle.load(open('movie_matrics.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_matrics = pickle.load(open('movie_matrics.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成User特征矩阵\n",
    "\n",
    "将训练好的用户特征组合成用户特征矩阵并保存到本地\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_layer_model = keras.models.Model(\n",
    "    inputs=[\n",
    "        mv_net.model.input[0], mv_net.model.input[1], mv_net.model.input[2],\n",
    "        mv_net.model.input[3]\n",
    "    ],\n",
    "    outputs=mv_net.model.get_layer(\"user_combine_layer_flat\").output)\n",
    "users_matrics = []\n",
    "\n",
    "for item in users.values:\n",
    "\n",
    "    user_combine_layer_flat_val = user_layer_model([\n",
    "        np.reshape(item.take(0), [1, 1]),\n",
    "        np.reshape(item.take(1), [1, 1]),\n",
    "        np.reshape(item.take(2), [1, 1]),\n",
    "        np.reshape(item.take(3), [1, 1])\n",
    "    ])\n",
    "    users_matrics.append(user_combine_layer_flat_val)\n",
    "\n",
    "pickle.dump((np.array(users_matrics).reshape(-1, 200)),\n",
    "            open('users_matrics.p', 'wb'))\n",
    "users_matrics = pickle.load(open('users_matrics.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_matrics = pickle.load(open('users_matrics.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 开始推荐电影\n",
    "\n",
    "使用生产的用户特征矩阵和电影特征矩阵做电影推荐\n",
    "\n",
    "### 推荐同类型的电影\n",
    "\n",
    "思路是计算当前看的电影特征向量与整个电影特征矩阵的余弦相似度，取相似度最大的top_k个，这里加了些随机选择在里面，保证每次的推荐稍稍有些不同。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_same_type_movie(movie_id_val, top_k=20):\n",
    "\n",
    "    norm_movie_matrics = tf.sqrt(\n",
    "        tf.reduce_sum(tf.square(movie_matrics), 1, keepdims=True))\n",
    "    normalized_movie_matrics = movie_matrics / norm_movie_matrics\n",
    "\n",
    "    #推荐同类型的电影\n",
    "    probs_embeddings = (movie_matrics[movieid2idx[movie_id_val]]).reshape(\n",
    "        [1, 200])\n",
    "    probs_similarity = tf.matmul(probs_embeddings,\n",
    "                                 tf.transpose(normalized_movie_matrics))\n",
    "    sim = (probs_similarity.numpy())\n",
    "    #     results = (-sim[0]).argsort()[0:top_k]\n",
    "    #     print(results)\n",
    "\n",
    "    print(\"您看的电影是：{}\".format(movies_orig[movieid2idx[movie_id_val]]))\n",
    "    print(\"以下是给您的推荐：\")\n",
    "    p = np.squeeze(sim)\n",
    "    p[np.argsort(p)[:-top_k]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    results = set()\n",
    "    while len(results) != 5:\n",
    "        c = np.random.choice(3883, 1, p=p)[0]\n",
    "        results.add(c)\n",
    "    for val in (results):\n",
    "        print(val)\n",
    "        print(movies_orig[val])\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "您看的电影是：[1401 'Ghosts of Mississippi (1996)' 'Drama']\n",
      "以下是给您的推荐：\n",
      "1380\n",
      "[1401 'Ghosts of Mississippi (1996)' 'Drama']\n",
      "3301\n",
      "[3370 'Betrayed (1988)' 'Drama|Thriller']\n",
      "1557\n",
      "[1598 'Desperate Measures (1998)' 'Crime|Drama|Thriller']\n",
      "792\n",
      "[802 'Phenomenon (1996)' 'Drama|Romance']\n",
      "346\n",
      "[350 'Client, The (1994)' 'Drama|Mystery|Thriller']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{346, 792, 1380, 1557, 3301}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommend_same_type_movie(1401, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 推荐您喜欢的电影\n",
    "\n",
    "思路是使用用户特征向量与电影特征矩阵计算所有电影的评分，取评分最高的top_k个，同样加了些随机选择部分\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_your_favorite_movie(user_id_val, top_k=10):\n",
    "\n",
    "    #推荐您喜欢的电影\n",
    "    probs_embeddings = (users_matrics[user_id_val - 1]).reshape([1, 200])\n",
    "\n",
    "    probs_similarity = tf.matmul(probs_embeddings, tf.transpose(movie_matrics))\n",
    "    sim = (probs_similarity.numpy())\n",
    "    #     print(sim.shape)\n",
    "    #     results = (-sim[0]).argsort()[0:top_k]\n",
    "    #     print(results)\n",
    "\n",
    "    #     sim_norm = probs_norm_similarity.eval()\n",
    "    #     print((-sim_norm[0]).argsort()[0:top_k])\n",
    "\n",
    "    print(\"以下是给您的推荐：\")\n",
    "    p = np.squeeze(sim)\n",
    "    p[np.argsort(p)[:-top_k]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    results = set()\n",
    "    while len(results) != 5:\n",
    "        c = np.random.choice(3883, 1, p=p)[0]\n",
    "        results.add(c)\n",
    "    for val in (results):\n",
    "        print(val)\n",
    "        print(movies_orig[val])\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "以下是给您的推荐：\n",
      "3846\n",
      "[3916 'Remember the Titans (2000)' 'Drama']\n",
      "145\n",
      "[147 'Basketball Diaries, The (1995)' 'Drama']\n",
      "2133\n",
      "[2202 'Lifeboat (1944)' 'Drama|Thriller|War']\n",
      "2263\n",
      "[2332 'Belly (1998)' 'Crime|Drama']\n",
      "315\n",
      "[318 'Shawshank Redemption, The (1994)' 'Drama']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{145, 315, 2133, 2263, 3846}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommend_your_favorite_movie(234, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 看过这个电影的人还看了（喜欢）哪些电影\n",
    "- 首先选出喜欢某个电影的top_k个人，得到这几个人的用户特征向量。\n",
    "- 然后计算这几个人对所有电影的评分\n",
    "- 选择每个人评分最高的电影作为推荐\n",
    "- 同样加入了随机选择"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def recommend_other_favorite_movie(movie_id_val, top_k=20):\n",
    "\n",
    "    probs_movie_embeddings = (\n",
    "        movie_matrics[movieid2idx[movie_id_val]]).reshape([1, 200])\n",
    "    probs_user_favorite_similarity = tf.matmul(probs_movie_embeddings,\n",
    "                                               tf.transpose(users_matrics))\n",
    "    favorite_user_id = np.argsort(\n",
    "        probs_user_favorite_similarity.numpy())[0][-top_k:]\n",
    "    #     print(normalized_users_matrics.numpy().shape)\n",
    "    #     print(probs_user_favorite_similarity.numpy()[0][favorite_user_id])\n",
    "    #     print(favorite_user_id.shape)\n",
    "\n",
    "    print(\"您看的电影是：{}\".format(movies_orig[movieid2idx[movie_id_val]]))\n",
    "\n",
    "    print(\"喜欢看这个电影的人是：{}\".format(users_orig[favorite_user_id - 1]))\n",
    "    probs_users_embeddings = (users_matrics[favorite_user_id - 1]).reshape(\n",
    "        [-1, 200])\n",
    "    probs_similarity = tf.matmul(probs_users_embeddings,\n",
    "                                 tf.transpose(movie_matrics))\n",
    "    sim = (probs_similarity.numpy())\n",
    "    #     results = (-sim[0]).argsort()[0:top_k]\n",
    "    #     print(results)\n",
    "\n",
    "    #     print(sim.shape)\n",
    "    #     print(np.argmax(sim, 1))\n",
    "    p = np.argmax(sim, 1)\n",
    "    print(\"喜欢看这个电影的人还喜欢看：\")\n",
    "\n",
    "    if len(set(p)) < 5:\n",
    "        results = set(p)\n",
    "    else:\n",
    "        results = set()\n",
    "        while len(results) != 5:\n",
    "            c = p[random.randrange(top_k)]\n",
    "            results.add(c)\n",
    "    for val in (results):\n",
    "        print(val)\n",
    "        print(movies_orig[val])\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "您看的电影是：[1401 'Ghosts of Mississippi (1996)' 'Drama']\n",
      "喜欢看这个电影的人是：[[2517 'F' 35 9]\n",
      " [5690 'M' 18 4]\n",
      " [4849 'F' 18 4]\n",
      " [4127 'M' 50 17]\n",
      " [1880 'M' 35 0]\n",
      " [4800 'M' 18 4]\n",
      " [2390 'F' 25 6]\n",
      " [2696 'M' 25 7]\n",
      " [5050 'F' 18 4]\n",
      " [2065 'M' 25 6]\n",
      " [3764 'M' 25 1]\n",
      " [767 'M' 25 12]\n",
      " [4593 'F' 45 1]\n",
      " [100 'M' 35 17]\n",
      " [1415 'M' 45 14]\n",
      " [2294 'M' 56 13]\n",
      " [5296 'F' 1 0]\n",
      " [774 'M' 18 4]\n",
      " [4503 'M' 56 1]\n",
      " [4518 'M' 25 0]]\n",
      "喜欢看这个电影的人还喜欢看：\n",
      "643\n",
      "[649 'Cold Fever (� k鰈dum klaka) (1994)' 'Comedy|Drama']\n",
      "3822\n",
      "[3892 'Anatomy (Anatomie) (2000)' 'Horror']\n",
      "2995\n",
      "[3064 'Poison Ivy: New Seduction (1997)' 'Thriller']\n",
      "1180\n",
      "[1198 'Raiders of the Lost Ark (1981)' 'Action|Adventure']\n",
      "1790\n",
      "[1859 'Taste of Cherry (1997)' 'Drama']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{643, 1180, 1790, 2995, 3822}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommend_other_favorite_movie(1401, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个结果里面20个人最喜欢这两个电影，所以只输出了两个结果"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
